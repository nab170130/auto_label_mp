{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "AutoLabeling Plotter.ipynb",
      "provenance": [],
      "toc_visible": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EQ1QwA3w3j2K"
      },
      "source": [
        "# SETUP"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BxZP_ef33osL"
      },
      "source": [
        "## Repo Installation"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xPtVTjcV3oRT"
      },
      "source": [
        "import sys\n",
        "\n",
        "# Submodlib for DISTIL; utilize oom_fix branch instead of default pip installation.\n",
        "!git clone https://github.com/decile-team/submodlib.git\n",
        "!cd submodlib && git checkout oom_fix\n",
        "!cd submodlib && pip install -r requirements.txt\n",
        "!cd submodlib && python setup.py bdist_wheel\n",
        "!cd submodlib && pip install .\n",
        "\n",
        "# DISTIL\n",
        "!git clone https://github.com/decile-team/distil.git\n",
        "sys.path.append(\"/content/distil/\")\n",
        "\n",
        "# Plotting utilities\n",
        "!sudo apt-get update\n",
        "!sudo apt update\n",
        "!sudo apt-get install texlive-latex-recommended\n",
        "!sudo apt install texlive-latex-extra\n",
        "!sudo apt install dvipng\n",
        "!sudo apt install cm-super"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zDUywR6c3sRU"
      },
      "source": [
        "## Imports"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gczdlbmL3skQ"
      },
      "source": [
        "import copy\n",
        "import csv\n",
        "import gc\n",
        "import json\n",
        "import math\n",
        "import matplotlib\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "import os\n",
        "import pickle\n",
        "import re\n",
        "import sklearn\n",
        "import submodlib\n",
        "import sys\n",
        "import time\n",
        "import torch\n",
        "import torch.multiprocessing as mp\n",
        "import torch.optim as optim\n",
        "import zipfile\n",
        "\n",
        "from distil.active_learning_strategies.badge import BADGE\n",
        "from distil.active_learning_strategies.entropy_sampling import EntropySampling\n",
        "from distil.active_learning_strategies.least_confidence_sampling import LeastConfidenceSampling\n",
        "from distil.active_learning_strategies.margin_sampling import MarginSampling\n",
        "from distil.active_learning_strategies.partition_strategy import PartitionStrategy as ALPartitionStrategy\n",
        "from distil.active_learning_strategies.random_sampling import RandomSampling\n",
        "from distil.active_learning_strategies.strategy import Strategy\n",
        "from distil.utils.models import MnistNet, ResNet18\n",
        "from distil.utils.train_helper import data_train\n",
        "from distil.utils.utils import LabeledToUnlabeledDataset\n",
        "\n",
        "from PIL import Image\n",
        "\n",
        "from scipy.io import loadmat\n",
        "\n",
        "from google.colab import drive\n",
        "from google.colab import files\n",
        "drive.mount(\"/content/drive\")\n",
        "\n",
        "from torch import nn, Tensor\n",
        "from torch.utils.data import ConcatDataset, DataLoader, Dataset, Subset\n",
        "\n",
        "from torchvision import datasets, transforms\n",
        "from torchvision.datasets.utils import download_and_extract_archive\n",
        "from torchvision._internally_replaced_utils import load_state_dict_from_url\n",
        "\n",
        "from typing import Type, Any, Callable, Union, List, Optional"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wSJxwLry77EX"
      },
      "source": [
        "## Saving Parameters"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_jwXBzSg77Uh"
      },
      "source": [
        "mount_point_directory = \"/content/drive/\"\n",
        "drive.mount(mount_point_directory)\n",
        "mount_point_directory = os.path.join(mount_point_directory, \"MyDrive\")\n",
        "\n",
        "google_drive_directory = \"auto_labeling_experiments/results/\"\n",
        "base_save_directory = os.path.join(mount_point_directory, google_drive_directory)\n",
        "\n",
        "google_drive_directory = \"AL_HC_Auto/Human_Expts/\"\n",
        "base_labeling_directory = os.path.join(mount_point_directory, google_drive_directory)\n",
        "\n",
        "google_drive_directory = \"auto_labeling_experiments/model/\"\n",
        "model_directory = os.path.join(mount_point_directory, google_drive_directory)\n",
        "\n",
        "dataset_root_directory = \"/content/\"\n",
        "\n",
        "os.makedirs(model_directory, exist_ok=True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "92H3T1WTZv-W"
      },
      "source": [
        "## Pre-Experiment"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kOLNKbBFZwNK"
      },
      "source": [
        "args = {'islogs': False,\n",
        "        'optimizer': 'sgd',\n",
        "        'isverbose': True,\n",
        "        'isreset': True,\n",
        "        'max_accuracy': 0.99,\n",
        "        'n_epoch': 300,\n",
        "        'lr': 0.001,\n",
        "        'device': 'cuda',\n",
        "        'batch_size': 64,\n",
        "        'thread_count': 3,\n",
        "        'metric': 'cosine',\n",
        "        'embedding_type': 'gradients',\n",
        "        'gradType': 'bias_linear'}"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# EXPERIMENT"
      ],
      "metadata": {
        "id": "X0FyNdUVhX81"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Definitions"
      ],
      "metadata": {
        "id": "1tMZjN5mhZ81"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5Z5HBwIQVmXr"
      },
      "source": [
        "### Evaluation Utilities"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EBaXso2GVmm_"
      },
      "source": [
        "def get_label_counts(dataset, nclasses, batch_size=64):\n",
        "\n",
        "    label_counts = [0 for x in range(nclasses)]\n",
        "    dataloader = DataLoader(dataset, shuffle=False, batch_size=batch_size)\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for batch_idx, (data, labels) in enumerate(dataloader):\n",
        "            for cls in range(nclasses):\n",
        "                count = len(torch.where(labels==cls)[0])\n",
        "                label_counts[cls] += count\n",
        "\n",
        "    return label_counts\n",
        "\n",
        "def get_labels(dataset, batch_size=64):\n",
        "\n",
        "    dataloader = DataLoader(dataset, shuffle=False, batch_size=batch_size)\n",
        "    \n",
        "    all_labels = []\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for batch_idx, (data, labels) in enumerate(dataloader):\n",
        "            all_labels.extend(labels)\n",
        "\n",
        "    return torch.tensor(all_labels)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LdNFo8oDddam"
      },
      "source": [
        "### Experiment Fixture Creation"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jjpiJk4xddpS"
      },
      "source": [
        "def get_tiny_imagenet(dataset_root_path):\n",
        "\n",
        "    # Download and extract TinyImageNet if it isn't already.\n",
        "    filepath = os.path.join(dataset_root_directory, \"tiny-imagenet-200.zip\")\n",
        "    if not os.path.exists(filepath):\n",
        "        download_command = F\"wget -P {dataset_root_directory} http://cs231n.stanford.edu/tiny-imagenet-200.zip\"\n",
        "        os.system(download_command)\n",
        "\n",
        "    dataset_path = os.path.join(dataset_root_directory, \"tiny-imagenet-200\")\n",
        "    if not os.path.exists(dataset_path):\n",
        "        with zipfile.ZipFile(filepath, 'r') as zip_ref:\n",
        "            zip_ref.extractall(dataset_root_directory)\n",
        "\n",
        "    # TinyImageNet has a test set, but it's labels are not available (following good practice).\n",
        "    # Hence, we must evaluate on the validation set. We prepare the validation set according to \n",
        "    # https://towardsdatascience.com/pytorch-ignite-classifying-tiny-imagenet-with-efficientnet-e5b1768e5e8f\n",
        "    # so that PyTorch's ImageFolder class can be used.\n",
        "    validation_dir = os.path.join(dataset_path, 'val')\n",
        "\n",
        "    # Open and read val annotations text file\n",
        "    with open(os.path.join(validation_dir, 'val_annotations.txt'), 'r') as fp:\n",
        "        data = fp.readlines()\n",
        "\n",
        "    # Create image filename to class dictionary\n",
        "    val_image_filename_to_class_dict = {}\n",
        "    for line in data:\n",
        "        words = line.split('\\t')\n",
        "        val_image_filename_to_class_dict[words[0]] = words[1]\n",
        "\n",
        "    # Map each image into its own class folder\n",
        "    old_val_img_dir = os.path.join(validation_dir, 'images')\n",
        "    for img, folder in val_image_filename_to_class_dict.items():\n",
        "        newpath = (os.path.join(validation_dir, folder, 'images'))\n",
        "        if not os.path.exists(newpath):\n",
        "            os.makedirs(newpath)\n",
        "        if os.path.exists(os.path.join(old_val_img_dir, img)):\n",
        "            os.rename(os.path.join(old_val_img_dir, img), os.path.join(newpath, img))\n",
        "    if os.path.exists(old_val_img_dir):\n",
        "        os.rmdir(old_val_img_dir)\n",
        "\n",
        "def get_experiment_fixture(dataset_root_path, dataset_name, seed_set_size, model_name, model_base_path, init_model_train_args):\n",
        "\n",
        "    # Load the dataset\n",
        "    if dataset_name == \"CIFAR10\":\n",
        "\n",
        "        train_transform = transforms.Compose([transforms.RandomCrop(32, padding=4), transforms.RandomHorizontalFlip(), transforms.ToTensor(), transforms.Normalize((0.4914, 0.4822, 0.4465), (0.2023, 0.1994, 0.2010))])\n",
        "        test_transform = transforms.Compose([transforms.ToTensor(), transforms.Normalize((0.4914, 0.4822, 0.4465), (0.2023, 0.1994, 0.2010))])\n",
        "\n",
        "        full_train_dataset = datasets.CIFAR10(dataset_root_path, download=True, train=True, transform=train_transform)\n",
        "        test_dataset = datasets.CIFAR10(dataset_root_path, download=True, train=False, transform=test_transform)\n",
        "\n",
        "        nclasses = 10 # NUM CLASSES HERE\n",
        "\n",
        "    elif dataset_name == \"CIFAR100\":\n",
        "\n",
        "        train_transform = transforms.Compose([transforms.RandomCrop(32, padding=4), transforms.RandomHorizontalFlip(), transforms.ToTensor(), transforms.Normalize((0.5071, 0.4867, 0.4408), (0.2675, 0.2565, 0.2761))])\n",
        "        test_transform = transforms.Compose([transforms.ToTensor(), transforms.Normalize((0.5071, 0.4867, 0.4408), (0.2675, 0.2565, 0.2761))])\n",
        "\n",
        "        full_train_dataset = datasets.CIFAR100(dataset_root_path, download=True, train=True, transform=train_transform)\n",
        "        test_dataset = datasets.CIFAR100(dataset_root_path, download=True, train=False, transform=test_transform)\n",
        "\n",
        "        nclasses = 100 # NUM CLASSES HERE\n",
        "\n",
        "    elif dataset_name == \"MNIST\":\n",
        "\n",
        "        image_dim=28\n",
        "        train_transform = transforms.Compose([transforms.Resize((image_dim, image_dim)), transforms.ToTensor(), transforms.Normalize((0.1307,), (0.3081,))])\n",
        "        test_transform = transforms.Compose([transforms.Resize((image_dim, image_dim)), transforms.ToTensor(), transforms.Normalize((0.1307,), (0.3081,))])\n",
        "\n",
        "        full_train_dataset = datasets.MNIST(dataset_root_path, download=True, train=True, transform=train_transform)\n",
        "        test_dataset = datasets.MNIST(dataset_root_path, download=True, train=False, transform=test_transform)\n",
        "\n",
        "        nclasses = 10 # NUM CLASSES HERE\n",
        "\n",
        "    elif dataset_name == \"TinyImageNet\":\n",
        "\n",
        "        get_tiny_imagenet(dataset_root_path)\n",
        "\n",
        "        train_transform = transforms.Compose([transforms.RandomCrop(64, padding=4), transforms.RandomHorizontalFlip(), transforms.ToTensor(), transforms.Normalize((0.485, 0.456, 0.406), (0.229, 0.224, 0.225))])\n",
        "        test_transform = transforms.Compose([transforms.ToTensor(), transforms.Normalize((0.485, 0.456, 0.406), (0.229, 0.224, 0.225))]) # ImageNet mean/std\n",
        "\n",
        "        # Use val as test\n",
        "        train_path = os.path.join(dataset_root_path, \"tiny-imagenet-200\", \"train\")\n",
        "        test_path = os.path.join(dataset_root_path, \"tiny-imagenet-200\", \"val\")\n",
        "        full_train_dataset = datasets.ImageFolder(train_path, transform=train_transform)\n",
        "        test_dataset = datasets.ImageFolder(test_path, transform=test_transform)\n",
        "\n",
        "        nclasses = 200 \n",
        "\n",
        "    if model_name == \"resnet18\":\n",
        "        model = ResNet18(num_classes=nclasses)\n",
        "    elif model_name == \"mnistnet\":\n",
        "        model = MnistNet()\n",
        "    else:\n",
        "        raise ValueError(\"Add model implementation\")\n",
        "\n",
        "    # Seed the rng used in dataset splits\n",
        "    np.random.seed(42)\n",
        "\n",
        "    # Retrieve the labels of the training set\n",
        "    train_labels = get_labels(full_train_dataset)\n",
        "\n",
        "    # Derive a list of indices that will represent the training set indices. The rest will represent the unlabeled set indices.\n",
        "    per_class_size = seed_set_size // nclasses\n",
        "    initial_train_idx = []\n",
        "    for cls in range(nclasses):\n",
        "\n",
        "        # Sample random points per class to form a balanced seed\n",
        "        cls_idx = torch.where(train_labels==cls)[0]\n",
        "        chosen_idx = np.random.choice(cls_idx, size=per_class_size, replace=False)\n",
        "        initial_train_idx.extend(chosen_idx)\n",
        "\n",
        "    # See if a model has already been trained for this fixture.\n",
        "    model_name = F\"{dataset_name}_{model_name}_{seed_set_size}\"\n",
        "    model_save_path = os.path.join(model_base_path, model_name)\n",
        "\n",
        "    if os.path.isfile(model_save_path):\n",
        "        print(\"Found Initial Model\")\n",
        "        state_dict = torch.load(model_save_path)\n",
        "        model.load_state_dict(state_dict)\n",
        "    else:\n",
        "        print(\"Training Initial Model...\")\n",
        "        init_trainer = data_train(Subset(full_train_dataset, initial_train_idx), model, init_model_train_args)\n",
        "        model = init_trainer.train(None)\n",
        "        torch.save(model.state_dict(), model_save_path)\n",
        "\n",
        "    return full_train_dataset, test_dataset, initial_train_idx, model, nclasses"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Datasets"
      ],
      "metadata": {
        "id": "oL5xirN0yGMe"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class CarsDataset(Dataset):\n",
        "    \"\"\"\n",
        "    PyTorch interface for Stanford Cars-196 dataset. Thanks to https://github.com/dtiarks/pytorch_cars\n",
        "    for the skeleton.\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, root_directory, train=True, download=False, transform=None):\n",
        "        \"\"\"\n",
        "        Args:\n",
        "            mat_anno (string): Path to the MATLAB annotation file.\n",
        "            data_dir (string): Directory with all the images.\n",
        "            transform (callable, optional): Optional transform to be applied\n",
        "                on a sample.\n",
        "        \"\"\"\n",
        "\n",
        "        cars_root = os.path.join(root_directory, \"cars196\")\n",
        "        cars_meta = os.path.join(cars_root, \"devkit\", \"cars_meta.mat\")\n",
        "\n",
        "        # Download the dataset if needed.\n",
        "        if download:\n",
        "            archives = [(\"http://ai.stanford.edu/~jkrause/car196/cars_train.tgz\", \"cars_train.tgz\"),\n",
        "                        (\"http://ai.stanford.edu/~jkrause/car196/cars_test.tgz\", \"cars_test.tgz\"),\n",
        "                        (\"https://ai.stanford.edu/~jkrause/cars/car_devkit.tgz\", \"car_devkit.tgz\")]\n",
        "            for archive, file_to_check in archives:\n",
        "                check_path = os.path.join(cars_root, file_to_check)\n",
        "                if os.path.exists(check_path):\n",
        "                    continue\n",
        "                \n",
        "                download_and_extract_archive(archive, cars_root)\n",
        "\n",
        "        # Set annotations + data source depending on split\n",
        "        if train:\n",
        "            mat_anno = os.path.join(cars_root, \"devkit\", \"cars_train_annos.mat\")\n",
        "            self.data_dir = os.path.join(cars_root, \"cars_train\")\n",
        "        else:\n",
        "            mat_anno = os.path.join(cars_root, \"devkit\", \"cars_test_annos.mat\")\n",
        "            self.data_dir = os.path.join(cars_root, \"cars_test\")\n",
        "\n",
        "        self.full_data_set = loadmat(mat_anno)\n",
        "        self.car_annotations = self.full_data_set['annotations'][0]\n",
        "        self.car_names = np.array(loadmat(cars_meta)['class_names'][0])\n",
        "\n",
        "        self.transform = transform\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.car_annotations)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        img_name = os.path.join(self.data_dir, self.car_annotations[idx][-1][0])\n",
        "        image = Image.open(img_name)\n",
        "        car_class = self.car_annotations[idx][-2][0][0] - 1 # zero-index the class\n",
        "\n",
        "        if self.transform:\n",
        "            image = self.transform(image)\n",
        "\n",
        "        return image, car_class\n",
        "    \n",
        "class BirdsDataset(Dataset):\n",
        "\n",
        "    def __init__(self, root_directory, train=True, download=False, transform=None):\n",
        "\n",
        "        birds_root = os.path.join(root_directory, \"caltech_birds\")\n",
        "\n",
        "        # Download if needed\n",
        "        if download:\n",
        "            archives = [(\"http://www.vision.caltech.edu/visipedia-data/CUB-200-2011/CUB_200_2011.tgz\", \"CUB_200_2011\")]\n",
        "            for archive, file_to_check in archives:\n",
        "                check_path = os.path.join(birds_root, file_to_check)\n",
        "                if os.path.exists(check_path):\n",
        "                    continue\n",
        "                \n",
        "                download_and_extract_archive(archive, birds_root)\n",
        "\n",
        "        # Get ID-to-filename map\n",
        "        id_to_filename_map = {}\n",
        "        id_to_image_path = os.path.join(birds_root, \"CUB_200_2011\", \"images.txt\")\n",
        "        with open(id_to_image_path, \"r\") as map_file:\n",
        "            map_reader = csv.reader(map_file, delimiter = \" \")\n",
        "            for (image_id, rel_path) in map_reader:\n",
        "                id_to_filename_map[int(image_id)] = rel_path\n",
        "\n",
        "        # Get ID-to-class map\n",
        "        id_to_class_map = {}\n",
        "        id_to_class_path = os.path.join(birds_root, \"CUB_200_2011\", \"image_class_labels.txt\")\n",
        "        with open(id_to_class_path, \"r\") as map_file:\n",
        "            map_reader = csv.reader(map_file, delimiter = \" \")\n",
        "            for (image_id, class_label) in map_reader:\n",
        "                id_to_class_map[int(image_id)] = int(class_label) - 1  # Subtract 1 for pytorch labeling scheme.                \n",
        "\n",
        "        # Get the train-test split\n",
        "        train_test_split_path = os.path.join(birds_root, \"CUB_200_2011\", \"train_test_split.txt\")\n",
        "        split_subset = []\n",
        "        with open(train_test_split_path, \"r\") as split_file:\n",
        "            split_reader = csv.reader(split_file, delimiter = \" \")\n",
        "            for image_id, is_train_image in split_reader:\n",
        "                if int(is_train_image) and train:\n",
        "                    split_subset.append(int(image_id))\n",
        "                elif not int(is_train_image) and not train:\n",
        "                    split_subset.append(int(image_id))\n",
        "\n",
        "        # Get list of filepaths and corresponding classes\n",
        "        self.filepaths = []\n",
        "        self.classes = []\n",
        "        image_folder_root = os.path.join(birds_root, \"CUB_200_2011\", \"images\")\n",
        "        for image_id in split_subset:\n",
        "            self.filepaths.append(os.path.join(image_folder_root, id_to_filename_map[image_id]))\n",
        "            self.classes.append(id_to_class_map[image_id])\n",
        "\n",
        "        self.transform = transform\n",
        "\n",
        "    def __getitem__(self, index):\n",
        "\n",
        "        img_name = self.filepaths[index]\n",
        "        label = self.classes[index]\n",
        "        image = Image.open(img_name)\n",
        "\n",
        "        if self.transform is not None:\n",
        "            image = self.transform(image)\n",
        "\n",
        "        return image, label\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.classes)\n",
        "\n",
        "class DogsDataset(Dataset):\n",
        "\n",
        "    def __init__(self, root_directory, train=True, download=False, transform=None):\n",
        "\n",
        "        dogs_root = os.path.join(root_directory, \"stanford_dogs\")\n",
        "\n",
        "        # Download if needed\n",
        "        if download:\n",
        "            archives = [(\"http://vision.stanford.edu/aditya86/ImageNetDogs/images.tar\",\"images.tar\"),\n",
        "                        (\"http://vision.stanford.edu/aditya86/ImageNetDogs/annotation.tar\",\"annotation.tar\"),\n",
        "                        (\"http://vision.stanford.edu/aditya86/ImageNetDogs/lists.tar\",\"lists.tar\")]\n",
        "            for archive, file_to_check in archives:\n",
        "                check_path = os.path.join(dogs_root, file_to_check)\n",
        "                if os.path.exists(check_path):\n",
        "                    continue\n",
        "                \n",
        "                download_and_extract_archive(archive, dogs_root)\n",
        "\n",
        "        if train:\n",
        "            dataset_mat_path = os.path.join(dogs_root, \"train_list.mat\")\n",
        "        else:\n",
        "            dataset_mat_path = os.path.join(dogs_root, \"test_list.mat\")\n",
        "\n",
        "        dataset_mat = loadmat(dataset_mat_path)\n",
        "\n",
        "        self.filepaths = []\n",
        "        for file_name in dataset_mat['file_list']:\n",
        "            file_name = file_name[0][0]\n",
        "            filepath = os.path.join(dogs_root, \"Images\", file_name)\n",
        "            self.filepaths.append(filepath)\n",
        "\n",
        "        self.labels = []\n",
        "        for label in dataset_mat['labels']:\n",
        "            label = label[0] - 1\n",
        "            self.labels.append(label)\n",
        "\n",
        "        self.transform = transform\n",
        "\n",
        "    def __getitem__(self, index):\n",
        "\n",
        "        img_name = self.filepaths[index]\n",
        "        label = self.labels[index]\n",
        "        image = Image.open(img_name)\n",
        "\n",
        "        if self.transform is not None:\n",
        "            image = self.transform(image)\n",
        "\n",
        "        return image, label\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.labels)"
      ],
      "metadata": {
        "id": "ZKtMOBoLyGcu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Z9ZhFPYxOUz-"
      },
      "source": [
        "# PLOTTING"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dnzyXHeFOngI"
      },
      "source": [
        "## Definitions"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PzvMEVcLO7bt"
      },
      "source": [
        "### Default Plot Styling"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uMszmLTzO-BP"
      },
      "source": [
        "matplotlib.rcParams['text.usetex'] = True\n",
        "matplotlib.rcParams['axes.spines.right'] = False\n",
        "matplotlib.rcParams['axes.spines.top'] = False\n",
        "plt.rc('font', family='serif')\n",
        "plt.rc('xtick', labelsize=18)\n",
        "plt.rc('ytick', labelsize=18)\n",
        "matplotlib.rc('text', usetex=True)\n",
        "matplotlib.rcParams['text.latex.preamble']=[r\"\\usepackage{amsmath,amsfonts}\"]\n",
        "matplotlib.rcParams['text.latex.preamble']=[r\"\\usepackage{bm}\"]\n",
        "plt.rc('axes', linewidth=1)\n",
        "plt.rc('font', weight='bold')\n",
        "matplotlib.rcParams['text.latex.preamble'] = [r'\\boldmath']\n",
        "\n",
        "figdim = (12,6)\n",
        "figdpi = 120\n",
        "\n",
        "shade_alpha = 0.2\n",
        "axis_label_font_size = 18\n",
        "legend_font_size = 18\n",
        "plot_title_font_size = 18"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eEallKUk18iQ"
      },
      "source": [
        "### Load Experiment Results"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Labeling Experiments"
      ],
      "metadata": {
        "id": "wKlceRkj1umd"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def get_labeling_experiment_results(base_labeling_directory, dataset_name):\n",
        "\n",
        "    if dataset_name == \"uc_merced\":\n",
        "        folder_name = \"uc_merced\"\n",
        "    elif dataset_name == \"svhn\":\n",
        "        folder_name = \"SVHN\"\n",
        "    elif dataset_name == \"stl10\":\n",
        "        folder_name = \"STL10\"\n",
        "    elif dataset_name == \"cifar10\":\n",
        "        folder_name = \"CIFAR-10\"\n",
        "\n",
        "    # Get path of directory containing all csv files for that dataset\n",
        "    dataset_csv_path = os.path.join(base_labeling_directory, folder_name)\n",
        "\n",
        "    # Get all files in path\n",
        "    files_in_path = os.listdir(dataset_csv_path)\n",
        "\n",
        "    average_ratios = []\n",
        "    median_ratios = []\n",
        "\n",
        "    for file_in_path in files_in_path:\n",
        "\n",
        "        # Ignore files that aren't summaries\n",
        "        if \"summary\" not in file_in_path:\n",
        "            continue\n",
        "\n",
        "        # Get path to file\n",
        "        file_path = os.path.join(dataset_csv_path, file_in_path)\n",
        "\n",
        "        # Get a csv reader\n",
        "        with open(file_path, \"r\") as csvfile:\n",
        "            summary_reader = csv.reader(csvfile, delimiter=',')\n",
        "            for row in summary_reader:\n",
        "                if len(row) != 4 or (row[0] != '0' and row[0] != '1'):\n",
        "                    continue\n",
        "                \n",
        "                print(file_in_path, row)\n",
        "\n",
        "                change_flag = row[0]\n",
        "                avg_time = row[2]\n",
        "                med_time = row[3]\n",
        "\n",
        "                if change_flag is not None:\n",
        "                    if int(change_flag):\n",
        "                        avg_time_to_correct = float(avg_time)\n",
        "                        med_time_to_correct = float(med_time)\n",
        "                    else:\n",
        "                        avg_time_to_verify = float(avg_time)\n",
        "                        med_time_to_verify = float(med_time)\n",
        "        \n",
        "        average_ratios.append(avg_time_to_correct / avg_time_to_verify)\n",
        "        median_ratios.append(med_time_to_correct / med_time_to_verify)\n",
        "    \n",
        "    return average_ratios, median_ratios"
      ],
      "metadata": {
        "id": "_ta-ietA111g"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Method Experiments"
      ],
      "metadata": {
        "id": "XmBg-CjA1ySz"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ydsmUi6X1832"
      },
      "source": [
        "def get_experiment_results(auto_label_load_directory, dataset_name, al_strategy, seed_size, b_config):\n",
        "\n",
        "    # Get the list of filenames from the auto_label_load_directory\n",
        "    filenames = os.listdir(auto_label_load_directory)    \n",
        "\n",
        "    # File names take the form F\"{dataset}_{al_strategy}_{human_correct_strategy}_{auto_assign_strategy}_{b1}_{b2}_{b3}_{seed_size}_{rounds}_{run_count}.json\"\n",
        "    # What is not provided by this method is the human_correct_strategy, auto_assign_strategy, rounds, and run_count.\n",
        "    # When returning the experiment list, we want to mark the list of experiments by human_correct_strategy, auto_assign_strategy, rounds.\n",
        "    experiment_results = {}\n",
        "\n",
        "    for filename in filenames:\n",
        "\n",
        "        # Get filename fields\n",
        "        if \"highest_confidence\" in filename:\n",
        "            filename_field_array = filename.split(\".\")[0].split(\"_highest_confidence_\")\n",
        "            filename_fields = filename_field_array[0].split(\"_\") + [\"highest_confidence\"] + filename_field_array[1].split(\"_\")\n",
        "        else:\n",
        "            filename_fields = filename.split(\".\")[0].split(\"_\")\n",
        "\n",
        "        # Ensure filename fields match the fixed fields\n",
        "        if dataset_name != filename_fields[0] or al_strategy != filename_fields[1] or seed_size != int(filename_fields[7]):\n",
        "            print(filename_fields)\n",
        "            continue\n",
        "\n",
        "        # Ensure b2, b3 fields in filename are in b_config list\n",
        "        if (int(filename_fields[4]), int(filename_fields[5]), int(filename_fields[6]), filename_fields[10] == \"True\") not in b_config:\n",
        "            continue\n",
        "\n",
        "        # If execution reaches here, then the filename is an experiment we want to include. We mark it by its (human_correct_strategy, auto_assign_strategy,\n",
        "        # b2, b3, rounds, is_adaptive) tuple.\n",
        "        exp_tuple = (filename_fields[2], filename_fields[3], int(filename_fields[4]), int(filename_fields[5]), int(filename_fields[6]), int(filename_fields[8]), filename_fields[10] == \"True\")\n",
        "        experiment_results_path = os.path.join(auto_label_load_directory, filename)\n",
        "        \n",
        "        # Get the json, putting it in the experiment results dictionary.\n",
        "        with open(experiment_results_path, \"r\") as json_file:\n",
        "            exp_dict = json.load(json_file)\n",
        "            \n",
        "            if exp_tuple not in experiment_results:\n",
        "                experiment_results[exp_tuple] = [exp_dict]\n",
        "            else:\n",
        "                experiment_results[exp_tuple].append(exp_dict)\n",
        "\n",
        "    # In each dictionary, keep only as many rounds as all dicts have.\n",
        "    for key in experiment_results:\n",
        "        max_rounds = float('inf')\n",
        "        experiment_dict_list = experiment_results[key]\n",
        "\n",
        "        for experiment_dict in experiment_dict_list:\n",
        "            round_count = len(experiment_dict['set_sizes']) - 1 # -1 for initial round\n",
        "            max_rounds = min(max_rounds, round_count)\n",
        "\n",
        "        for index, exp_dict in enumerate(experiment_dict_list):\n",
        "            exp_dict['auto_assigned_selected_idx']          = exp_dict['auto_assigned_selected_idx'][:max_rounds]\n",
        "            exp_dict['human_corrected_selected_idx']        = exp_dict['human_corrected_selected_idx'][:max_rounds]\n",
        "            exp_dict['active_learning_selected_idx']        = exp_dict['active_learning_selected_idx'][:max_rounds]\n",
        "            exp_dict['auto_assigned_selection_matrices']    = exp_dict['auto_assigned_selection_matrices'][:max_rounds]\n",
        "            exp_dict['human_corrected_selection_matrices']  = exp_dict['human_corrected_selection_matrices'][:max_rounds]\n",
        "            exp_dict['auto_assign_selection_times']         = exp_dict['auto_assign_selection_times'][:max_rounds]\n",
        "            exp_dict['human_correct_selection_times']       = exp_dict['human_correct_selection_times'][:max_rounds]\n",
        "            exp_dict['al_selection_times']                  = exp_dict['al_selection_times'][:max_rounds]\n",
        "            exp_dict['test_accuracies']                     = exp_dict['test_accuracies'][:max_rounds+1]\n",
        "            exp_dict['set_sizes']                           = exp_dict['set_sizes'][:max_rounds+1]\n",
        "            exp_dict['train_times']                         = exp_dict['train_times'][:max_rounds]\n",
        "            experiment_dict_list[index] = exp_dict\n",
        "\n",
        "        experiment_results[key] = experiment_dict_list\n",
        "\n",
        "    return experiment_results"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_BdaKBVj963K"
      },
      "source": [
        "### Averages/STDs of Experiments"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OewhnSyb-kiy"
      },
      "source": [
        "**Averages/STDs of Lists**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1fcBEaM-97QX"
      },
      "source": [
        "def get_avg_std(list_of_lists):\n",
        "\n",
        "    # Calculate average list\n",
        "    avg_list = None\n",
        "    for a_list in list_of_lists:\n",
        "        if avg_list is None:\n",
        "            avg_list = a_list\n",
        "        else:\n",
        "            avg_list = [(x + y) for (x,y) in zip(avg_list, a_list)]\n",
        "    num_lists = len(list_of_lists)\n",
        "    avg_list = [(x / num_lists) for x in avg_list]\n",
        "\n",
        "    if num_lists == 1:\n",
        "        std_list = [0.0 for x in range(len(avg_list))]\n",
        "        return avg_list, std_list\n",
        "\n",
        "    # Calculate sample standard dev. list\n",
        "    std_list = None\n",
        "    for a_list in list_of_lists:\n",
        "        to_add_list = [(x - y)*(x - y) for (x,y) in zip(a_list, avg_list)]\n",
        "        if std_list is None:\n",
        "            std_list = to_add_list\n",
        "        else:\n",
        "            std_list = [(x + y) for (x,y) in zip(to_add_list, std_list)]\n",
        "    std_list = [math.sqrt(x / (num_lists - 1)) for x in std_list]\n",
        "    \n",
        "    return avg_list, std_list"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kOw0y0-4-oog"
      },
      "source": [
        "#### Get Average/STD of Test Accuracies"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "K7AughDE-o1z"
      },
      "source": [
        "def get_avg_std_test_acc(list_of_exps):\n",
        "\n",
        "    list_of_lists = []\n",
        "    for exp in list_of_exps:\n",
        "        list_of_lists.append(exp['test_accuracies'])\n",
        "    return get_avg_std(list_of_lists)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uQLSCOnKAiwM"
      },
      "source": [
        "#### Get Average/STD of % Correctly Auto-Labeled Points"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "85JmhxtpAi9-"
      },
      "source": [
        "def get_avg_std_correctly_auto_labeled_points(list_of_exps):\n",
        "\n",
        "    list_of_lists = []\n",
        "    for exp in list_of_exps:\n",
        "        selection_matrices = exp['auto_assigned_selection_matrices']\n",
        "        exp_correctly_labeled_points_list = []\n",
        "        for selection_matrix in selection_matrices:\n",
        "            nclasses = len(selection_matrix)\n",
        "            correctly_labeled_points_frac = 0.\n",
        "            for i in range(nclasses):\n",
        "                correctly_labeled_points_frac += selection_matrix[i][i]\n",
        "            if sum([sum(selection_matrix_row) for selection_matrix_row in selection_matrix]) > 0:\n",
        "                correctly_labeled_points_frac /= sum([sum(selection_matrix_row) for selection_matrix_row in selection_matrix])\n",
        "            exp_correctly_labeled_points_list.append(correctly_labeled_points_frac)\n",
        "        list_of_lists.append(exp_correctly_labeled_points_list)\n",
        "    return get_avg_std(list_of_lists)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Get Average/STD of % Correctly Suggested Human-Corrected Points"
      ],
      "metadata": {
        "id": "3HEddfU4Vhq4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def get_avg_std_correctly_suggested_human_corrected_points(list_of_exps):\n",
        "\n",
        "    list_of_lists = []\n",
        "    for exp in list_of_exps:\n",
        "        selection_matrices = exp['human_corrected_selection_matrices']\n",
        "        exp_correctly_labeled_points_list = []\n",
        "        for selection_matrix in selection_matrices:\n",
        "            nclasses = len(selection_matrix)\n",
        "            correctly_labeled_points_frac = 0.\n",
        "            for i in range(nclasses):\n",
        "                correctly_labeled_points_frac += selection_matrix[i][i]\n",
        "            if sum([sum(selection_matrix_row) for selection_matrix_row in selection_matrix]) > 0:\n",
        "                correctly_labeled_points_frac /= sum([sum(selection_matrix_row) for selection_matrix_row in selection_matrix])\n",
        "            exp_correctly_labeled_points_list.append(correctly_labeled_points_frac)\n",
        "        list_of_lists.append(exp_correctly_labeled_points_list)\n",
        "    return get_avg_std(list_of_lists)"
      ],
      "metadata": {
        "id": "NQzEELauVh-6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Get Average/STD of % Correctly Suggested AL Points"
      ],
      "metadata": {
        "id": "XKz88aCvZVHy"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def get_avg_std_correctly_suggested_al_points(list_of_exps, al_pseudo_dataset):\n",
        "\n",
        "    experiment_suggestion_accuracies = []\n",
        "    for experiment in list_of_exps:\n",
        "\n",
        "        experiment_al_suggestion_accuracy = []\n",
        "        al_selected_points = experiment['active_learning_selected_idx']\n",
        "\n",
        "        for round in al_selected_points:\n",
        "            total_points_in_round = len(round)\n",
        "            total_correctly_suggested_points_in_round = 0\n",
        "            for selected_index, predicted_class in round:\n",
        "                _, ground_truth_label = al_pseudo_dataset[selected_index]\n",
        "                if ground_truth_label == predicted_class:\n",
        "                    total_correctly_suggested_points_in_round += 1\n",
        "            al_suggestion_acc_in_round = total_correctly_suggested_points_in_round / total_points_in_round\n",
        "            experiment_al_suggestion_accuracy.append(al_suggestion_acc_in_round)\n",
        "\n",
        "        experiment_suggestion_accuracies.append(experiment_al_suggestion_accuracy)\n",
        "    \n",
        "    return get_avg_std(experiment_suggestion_accuracies)"
      ],
      "metadata": {
        "id": "L6_GAN37ZVe0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2F57GyndiOgv"
      },
      "source": [
        "#### Get Average/STD of Cumulative Corrections Made"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Glar-bLkiOwI"
      },
      "source": [
        "def get_avg_std_total_corrections_needed(list_of_exps):\n",
        "\n",
        "    list_of_lists = []\n",
        "    for exp in list_of_exps:\n",
        "        selection_matrices = exp['human_corrected_selection_matrices']\n",
        "        exp_correctly_labeled_points_list = []\n",
        "        working_sum = 0\n",
        "        for selection_matrix in selection_matrices:\n",
        "            nclasses = len(selection_matrix)\n",
        "            corrections_needed = 0\n",
        "            for i in range(nclasses):\n",
        "                corrections_needed += sum(selection_matrix[i]) - selection_matrix[i][i]\n",
        "            working_sum += corrections_needed\n",
        "            exp_correctly_labeled_points_list.append(working_sum)\n",
        "        list_of_lists.append(exp_correctly_labeled_points_list)\n",
        "    return get_avg_std(list_of_lists)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fdOLd5IzoGze"
      },
      "source": [
        "#### Get Average/STD of # Incorrectly Labeled Points versus Score"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "P80JWvXeoHKZ"
      },
      "source": [
        "def get_avg_std_inc_label_score(exp_save_directory, dataset_root_directory, dataset_name, model_name, model_directory, selection_mode, seed_set_size, budget, alpha, beta, train_cap, per_exp_runs, args, round, budget_granularity=0.1, human_corrected=False):\n",
        "\n",
        "    full_dataset, test_dataset, init_train_idx, model, nclasses = get_experiment_fixture(dataset_root_directory, dataset_name, seed_set_size, model_name, model_directory, args)\n",
        "    list_of_exps = get_experiment_results(exp_save_directory, dataset_root_directory, dataset_name, model_name, model_directory, selection_mode, seed_set_size, budget, alpha, beta, train_cap, per_exp_runs, args)\n",
        "\n",
        "    list_of_lists = []\n",
        "    point_cutoffs = []\n",
        "    for exp in list_of_exps:\n",
        "        if human_corrected:\n",
        "            selection_matrix = exp['human_corrected_selected_idx'][round]\n",
        "        else:\n",
        "            selection_matrix = exp['auto_assigned_selected_idx'][round]\n",
        "\n",
        "        correctness_gain_list = []\n",
        "        for sel_class, selection_row in enumerate(selection_matrix):\n",
        "            if len(selection_row) == 0:\n",
        "                continue\n",
        "\n",
        "            max_gain = max(selection_row, key=lambda x:x[1])[1]\n",
        "            min_gain = min(selection_row, key=lambda x:x[1])[1]\n",
        "\n",
        "            for sel_idx, gain in selection_row:\n",
        "                _, true_label = full_dataset[sel_idx]\n",
        "                normalized_gain = (gain - min_gain) / (max_gain - min_gain)\n",
        "                correctness_gain_list.append((sel_class == true_label, normalized_gain))\n",
        "\n",
        "        correctness_gain_list = sorted(correctness_gain_list, key=lambda x:x[1])\n",
        "        budget = len(correctness_gain_list)\n",
        "        num_budget_slices = int(1 / budget_granularity)\n",
        "        point_cutoffs = [int(i * budget / num_budget_slices) for i in range(num_budget_slices + 1)]\n",
        "\n",
        "        total_computed_gain = 0.\n",
        "        incorrectly_labeled_total_list = []\n",
        "        for point_cutoff in point_cutoffs:\n",
        "            if point_cutoff == 0:\n",
        "                gain_cutoff = -1\n",
        "            else:\n",
        "                gain_cutoff = correctness_gain_list[point_cutoff - 1][1]\n",
        "\n",
        "            total_incorrect = 0\n",
        "\n",
        "            for correct, norm_gain in correctness_gain_list:\n",
        "                if norm_gain > gain_cutoff:\n",
        "                    break\n",
        "                if not correct:\n",
        "                    total_incorrect += 1\n",
        "            incorrectly_labeled_total_list.append(total_incorrect)\n",
        "        list_of_lists.append(incorrectly_labeled_total_list)\n",
        "\n",
        "    return point_cutoffs, get_avg_std(list_of_lists)                   "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZfMLWWyv1NIt"
      },
      "source": [
        "#### Get Labeling Cost over Experiment"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Dd8vLDZR1NW6"
      },
      "source": [
        "def get_avg_labeling_costs(list_of_exps, cost_to_check_label, cost_to_assign_label, use_al_pseudo=False, al_pseudo_dataset=None):\n",
        "\n",
        "    if use_al_pseudo and al_pseudo_dataset is None:\n",
        "        raise ValueError(\"Need to pass dataset as well\")\n",
        "\n",
        "    list_of_lists = []\n",
        "    for exp in list_of_exps:\n",
        "        exp_labeling_costs = [0]                                                        # Initial round has no labeling cost\n",
        "        human_corrected_selection_matrices = exp['human_corrected_selection_matrices']  # We need to know which labels were correct & which ones were incorrect when involving human\n",
        "        active_learning_selected_idx = exp['active_learning_selected_idx']              # We need to know how many active learning elements were selected\n",
        "        working_sum = 0\n",
        "\n",
        "        for round_num, selection_matrix in enumerate(human_corrected_selection_matrices):\n",
        "            nclasses = len(selection_matrix)\n",
        "\n",
        "            correctly_labeled_points = 0\n",
        "            incorrectly_labeled_points = 0\n",
        "            for i in range(nclasses):\n",
        "                correctly_labeled_points += selection_matrix[i][i]\n",
        "                incorrectly_labeled_points += sum(selection_matrix[i]) - selection_matrix[i][i]\n",
        "\n",
        "            # Determine if the AL selected in the round has pseudo-label info.\n",
        "            if len(active_learning_selected_idx[round_num]) > 0:\n",
        "                if len(active_learning_selected_idx[round_num][0]) == 2 and use_al_pseudo:\n",
        "                    # AL round has pseudo-label info. See which ones were correctly labeled, which ones weren't.\n",
        "                    for selected_index, predicted_class in active_learning_selected_idx[round_num]:\n",
        "                        _, ground_truth_label = al_pseudo_dataset[selected_index]\n",
        "                        if ground_truth_label == predicted_class:\n",
        "                            correctly_labeled_points += 1\n",
        "                        else:\n",
        "                            incorrectly_labeled_points += 1\n",
        "                else:\n",
        "                    # AL round does not have pseudo-label info\n",
        "                    incorrectly_labeled_points += len(active_learning_selected_idx[round_num])\n",
        "            \n",
        "            round_labeling_cost = correctly_labeled_points * cost_to_check_label + incorrectly_labeled_points * cost_to_assign_label\n",
        "            working_sum += round_labeling_cost\n",
        "            exp_labeling_costs.append(working_sum)\n",
        "\n",
        "        list_of_lists.append(exp_labeling_costs)\n",
        "    average_labeling_costs, std_labeling_costs = get_avg_std(list_of_lists)\n",
        "    return average_labeling_costs"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Determine AL Round Cutoff\n",
        "\n",
        "We purposefully run more AL rounds than necessary. Cut off AL rounds where it fits."
      ],
      "metadata": {
        "id": "zSzuJKc6mRmj"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def determine_al_round_cutoff(experiment_dictionary, full_train_dataset, c_v, c_a):\n",
        "\n",
        "    # Return: AL round cutoff.\n",
        "\n",
        "    # Get max labeling cost of non-AL method.\n",
        "    max_labeling_cost = -float('inf')\n",
        "    for (human_correct_strategy, auto_assign_strategy, b1, b2, b3, rounds, is_adaptive) in experiment_dictionary:\n",
        "\n",
        "        if b2 == 0 and b3 == 0:\n",
        "            continue\n",
        "\n",
        "        exp_results_list = experiment_dictionary[(human_correct_strategy, auto_assign_strategy, b1, b2, b3, rounds, is_adaptive)]\n",
        "        labeling_costs = get_avg_labeling_costs(exp_results_list, c_v, c_a, True, full_train_dataset)\n",
        "        \n",
        "        if labeling_costs[-1] > max_labeling_cost:\n",
        "            max_labeling_cost = labeling_costs[-1]\n",
        "            \n",
        "    # Get AL round whose labeling cost is just before the max labeling cost.\n",
        "    for (human_correct_strategy, auto_assign_strategy, b1, b2, b3, rounds, is_adaptive) in experiment_dictionary:\n",
        "\n",
        "        if b2 != 0 or b3 != 0:\n",
        "            continue\n",
        "\n",
        "        exp_results_list = experiment_dictionary[(human_correct_strategy, auto_assign_strategy, b1, b2, b3, rounds, is_adaptive)]\n",
        "        labeling_costs = get_avg_labeling_costs(exp_results_list, c_v, c_a, True, full_train_dataset)\n",
        "        \n",
        "        for cutoff_index in range(len(labeling_costs)):\n",
        "            if labeling_costs[cutoff_index] > max_labeling_cost:\n",
        "                return cutoff_index + 1, cutoff_index + 1"
      ],
      "metadata": {
        "id": "xYFUnlAVmR5T"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Labeling Efficiency"
      ],
      "metadata": {
        "id": "S9hniN7t7PFX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def find_cusp(labeling_cost, acc_list, target_acc):\n",
        "\n",
        "    high_index = -1\n",
        "\n",
        "    for i,acc in enumerate(acc_list):\n",
        "        if acc >= target_acc:\n",
        "            high_index = i\n",
        "            break\n",
        "\n",
        "    # Corner case: If target acc is equal to first in list, return corresponding labeling cost.\n",
        "    if high_index == 0:\n",
        "        return labeling_cost[0]\n",
        "\n",
        "    if high_index == -1:\n",
        "        return None\n",
        "\n",
        "    acc_ratio = (target_acc - acc_list[high_index-1]) / (acc_list[high_index] - acc_list[high_index-1])\n",
        "\n",
        "    calc_budget = acc_ratio * labeling_cost[high_index] + (1 - acc_ratio) * labeling_cost[high_index - 1]\n",
        "\n",
        "    return calc_budget\n",
        "\n",
        "def produce_label_efficiency_data(base_labeling_cost, base_method_accuracies, compared_labeling_cost, compared_method_accuracies, samples_on_x_axis):\n",
        "\n",
        "    # To get label efficiency data, we first need to calculate a range of accuracies that are shared across all methods.\n",
        "\n",
        "    # Calculate the min of the max accuracies\n",
        "    max_base_acc = max(base_method_accuracies)\n",
        "    max_compared_acc = max(compared_method_accuracies)\n",
        "    max_lower_bound = min([max_base_acc, max_compared_acc])\n",
        "\n",
        "    # Calculate the min accuracy obtained\n",
        "    min_acc = min(min(base_method_accuracies), min(compared_method_accuracies))\n",
        "\n",
        "    # Calculate range of accuracies to generate\n",
        "    high_low_acc_diff = max_lower_bound - min_acc\n",
        "\n",
        "    # Generate a list whose granularity depends on the samples on the x_axis.\n",
        "    target_acc_grain = high_low_acc_diff / (samples_on_x_axis - 1) - 0.000000001  # subtract a very, very tiny amount to prevent floating point issues\n",
        "    target_acc_list = [min_acc + x * (target_acc_grain) for x in range(samples_on_x_axis)]    \n",
        "\n",
        "    # For each such accuracy, calculate the corresponding labeling costs needed to reach that accuracy.\n",
        "    # Note: find_cusp() uses linear interpolation to find labeling costs.\n",
        "    labeling_efficiencies = []\n",
        "    for target_accuracy in target_acc_list:\n",
        "        base_required_labeling_cost = find_cusp(base_labeling_cost, base_method_accuracies, target_accuracy)\n",
        "        comp_required_labeling_cost = find_cusp(compared_labeling_cost, compared_method_accuracies, target_accuracy)\n",
        "\n",
        "        # If indeterminate, append as 1.\n",
        "        if base_required_labeling_cost == 0 and comp_required_labeling_cost == 0:\n",
        "            labeling_efficiencies.append(1.)\n",
        "        else:\n",
        "            labeling_efficiency = base_required_labeling_cost / comp_required_labeling_cost\n",
        "            labeling_efficiencies.append(labeling_efficiency)\n",
        "\n",
        "    return target_acc_list, labeling_efficiencies"
      ],
      "metadata": {
        "id": "ejgNOvMf7P3S"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Plot Parameters"
      ],
      "metadata": {
        "id": "YjYq9HUpi4fa"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Fixed Parameters**"
      ],
      "metadata": {
        "id": "MQPU3-cJjslf"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "dataset_root_path = \"/content/datasets\"\n",
        "color_dictionary = {(\"badge\",False):                              (0,0,1),\n",
        "                    (\"badge\",True):                               (0,0.5,1),\n",
        "                    (\"entropy\",False):                            (0.6, 0.2, 0.2),\n",
        "                    (\"entropy\",True):                             (0.8, 0.4, 0.2),\n",
        "                    (\"badge\",\"logdetmi\",\"highest_confidence\"):    (0,1,1),\n",
        "                    (\"entropy\",\"logdetmi\",\"highest_confidence\"):  (1, 0.6, 0.2)}\n",
        "\n",
        "label_eff_acc_granularity = 30\n",
        "acronym = \"Clarifier\""
      ],
      "metadata": {
        "id": "UFAUN-58i4tN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**$b_1$,$b_2$, $b_3$, $adaptive$ Configs**"
      ],
      "metadata": {
        "id": "T3JbODI5jwSr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "b_configs = [(60,400,140,False)]"
      ],
      "metadata": {
        "id": "30oIyvHcjwo5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**$c_a,c_v$ Settings**"
      ],
      "metadata": {
        "id": "sgsn6VDczCUP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "cost_configs = [(4,1),\n",
        "                (6,1),\n",
        "                (8,1)]"
      ],
      "metadata": {
        "id": "NpwnEwkqzChi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## SUGGESTED LABEL VERIFICATION"
      ],
      "metadata": {
        "id": "Q5Fhx8SE6IZ-"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Ratio Summary"
      ],
      "metadata": {
        "id": "nKoYjX4I6RUo"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def avg(my_list):\n",
        "    return sum(my_list) / len(my_list)\n",
        "\n",
        "dataset_name = \"cifar10\"\n",
        "cifar10_average_ratios, cifar10_median_ratios = get_labeling_experiment_results(base_labeling_directory, dataset_name)\n",
        "\n",
        "dataset_name = \"svhn\"\n",
        "svhn_average_ratios, svhn_median_ratios = get_labeling_experiment_results(base_labeling_directory, dataset_name)\n",
        "\n",
        "dataset_name = \"stl10\"\n",
        "stl10_average_ratios, stl10_median_ratios = get_labeling_experiment_results(base_labeling_directory, dataset_name)\n",
        "\n",
        "dataset_name = \"uc_merced\"\n",
        "uc_merced_average_ratios, uc_merced_median_ratios = get_labeling_experiment_results(base_labeling_directory, dataset_name)\n",
        "\n",
        "labels = [r\"\\textbf{CIFAR-10}\", r\"\\textbf{SVHN}\", r\"\\textbf{STL-10}\", r\"\\textbf{UC Merced}\"]\n",
        "bar_width = 0.35\n",
        "x = np.arange(len(labels))\n",
        "\n",
        "figsize = (7.5,7.5)\n",
        "comparison_fig, ax = plt.subplots(figsize=figsize, dpi=120)\n",
        "\n",
        "average_of_averages = [avg(cifar10_average_ratios), avg(svhn_average_ratios), avg(stl10_average_ratios), avg(uc_merced_average_ratios)]\n",
        "average_of_medians = [avg(cifar10_median_ratios), avg(svhn_median_ratios), avg(stl10_median_ratios), avg(uc_merced_median_ratios)]\n",
        "\n",
        "rects1 = ax.bar(x - bar_width/2, average_of_averages, bar_width, label=\"Average\")\n",
        "rects2 = ax.bar(x + bar_width/2, average_of_medians, bar_width, label=\"Median\")\n",
        "\n",
        "ax.set_ylabel(r\"\\textbf{Average of Ratios}\", fontsize=axis_label_font_size)\n",
        "ax.set_xticks(x)\n",
        "ax.set_xticklabels(labels)\n",
        "ax.spines[\"top\"].set_visible(True)\n",
        "ax.spines[\"right\"].set_visible(True)\n",
        "ax.legend(fontsize=legend_font_size)\n",
        "\n",
        "for i in x:\n",
        "    plt.text(i - bar_width/2, average_of_averages[i] + 0.02, r\"\\textbf{\" + str(round(average_of_averages[i],1)) + r\"}\", ha = \"center\", fontsize=axis_label_font_size)\n",
        "    plt.text(i + bar_width/2, average_of_medians[i] + 0.02, r\"\\textbf{\" + str(round(average_of_medians[i],1)) + r\"}\", ha = \"center\", fontsize=axis_label_font_size)\n",
        "\n",
        "comparison_fig.tight_layout()"
      ],
      "metadata": {
        "id": "pdmYbauI6NTs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Strawman Gains"
      ],
      "metadata": {
        "id": "_04kMK1T6Vyf"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "nrows = 1\n",
        "ncols = 2\n",
        "\n",
        "c_a = 3\n",
        "c_v = 1\n",
        "\n",
        "figsize = (5.5 * ncols, 6.5 * nrows)\n",
        "comparison_fig, axes = plt.subplots(nrows=nrows, ncols=ncols, figsize=figsize, dpi=120, gridspec_kw = {'top':0.55,'wspace':0.3,'hspace':0.3})\n",
        "axes = [axes]\n",
        "\n",
        "# Note: Figures are not to have titles. The caption serves this purpose.\n",
        "\n",
        "# Set error bar alpha\n",
        "shade_alpha = 0.25\n",
        "\n",
        "legend_line_dictionary = {}\n",
        "\n",
        "# =========================================================================================================================================================\n",
        "#\n",
        "# ====================================================================== CIFAR-10 =========================================================================\n",
        "#\n",
        "# =========================================================================================================================================================\n",
        "\n",
        "# Note: b_configs is a list of tuples of the form (b_1, b_2, b_3, is_adaptive).\n",
        "dataset = \"cifar10\"\n",
        "seed_size = 1000\n",
        "b_configs = [(600,0,0,False)]\n",
        "\n",
        "full_train_dataset = datasets.CIFAR10(dataset_root_path, download=True, train=True)\n",
        "test_dataset = datasets.CIFAR10(dataset_root_path, download=True, train=False)\n",
        "nclasses = 10 # NUM CLASSES HERE\n",
        "    \n",
        "# Set the subplot title, y-axis, x-axis\n",
        "axes[0][0].set_title(r\"\\textbf{CIFAR-10}\", fontsize=plot_title_font_size)\n",
        "axes[0][0].set_xlabel(r\"\\textbf{Labeling Cost (\" + F\"{c_a},\" + F\"{c_v})\" + r\"}\", fontsize=axis_label_font_size)\n",
        "axes[0][0].set_ylabel(r\"\\textbf{Accuracy}\", fontsize=axis_label_font_size)\n",
        "\n",
        "axes[0][1].set_title(r\"\\textbf{CIFAR-10}\", fontsize=plot_title_font_size)\n",
        "axes[0][1].set_xlabel(r\"\\textbf{Test Accuracy}\", fontsize=axis_label_font_size)\n",
        "axes[0][1].set_ylabel(r\"\\textbf{Label Efficiency}\", fontsize=axis_label_font_size)\n",
        "\n",
        "# ===== BADGE =====\n",
        "al_strategy = \"badge\"\n",
        "\n",
        "# Get CIFAR-10 results.\n",
        "results_directory = os.path.join(base_save_directory, dataset, al_strategy, str(seed_size))\n",
        "experiment_results = get_experiment_results(results_directory, dataset, al_strategy, seed_size, b_configs)\n",
        "\n",
        "# Get the average labeling costs across experiments along with the test accuracies. Plot them.\n",
        "for (human_correct_strategy, auto_assign_strategy, b1, b2, b3, rounds, is_adaptive) in experiment_results:\n",
        "\n",
        "    # Attempt to generate a color if there is not already one.\n",
        "    if (al_strategy,False) not in color_dictionary or (al_strategy,True) not in color_dictionary:\n",
        "        raise ValueError(F\"Add color for {al_strategy}\")\n",
        "    else:\n",
        "        no_suggestion_draw_color = color_dictionary[(al_strategy,False)]\n",
        "        suggestion_draw_color = color_dictionary[(al_strategy,True)]\n",
        "\n",
        "    # PLOT WITHOUT USING AL SUGGESTIONS\n",
        "    exp_results_list = experiment_results[(human_correct_strategy, auto_assign_strategy, b1, b2, b3, rounds, is_adaptive)]\n",
        "    labeling_costs = get_avg_labeling_costs(exp_results_list, c_v, c_a, False, full_train_dataset)\n",
        "    average_acc, std = get_avg_std_test_acc(exp_results_list)\n",
        "    lower_list = [(x-y) for (x,y) in zip(average_acc,std)]\n",
        "    upper_list = [(x+y) for (x,y) in zip(average_acc,std)]\n",
        "    line = axes[0][0].plot(labeling_costs, average_acc, color=no_suggestion_draw_color, marker='o', linestyle=\"--\")[0]\n",
        "    axes[0][0].fill_between(labeling_costs, lower_list, upper_list, alpha=shade_alpha, color=no_suggestion_draw_color)\n",
        "    \n",
        "    legend_line_dictionary[(al_strategy,False)] = line\n",
        "\n",
        "    # PLOT USING AL SUGGESTIONS\n",
        "    labeling_costs = get_avg_labeling_costs(exp_results_list, c_v, c_a, True, full_train_dataset)\n",
        "    average_acc, std = get_avg_std_test_acc(exp_results_list)\n",
        "    lower_list = [(x-y) for (x,y) in zip(average_acc,std)]\n",
        "    upper_list = [(x+y) for (x,y) in zip(average_acc,std)]\n",
        "    line = axes[0][0].plot(labeling_costs, average_acc, color=suggestion_draw_color, marker='o')[0]\n",
        "    axes[0][0].fill_between(labeling_costs, lower_list, upper_list, alpha=shade_alpha, color=suggestion_draw_color)\n",
        "\n",
        "    legend_line_dictionary[(al_strategy,True)] = line\n",
        "\n",
        "# Do the same for the labeling efficiency.\n",
        "\n",
        "for (human_correct_strategy, auto_assign_strategy, b1, b2, b3, rounds, is_adaptive) in experiment_results:\n",
        "\n",
        "    if (al_strategy,False) not in color_dictionary or (al_strategy,True) not in color_dictionary:\n",
        "        raise ValueError(F\"Add color for {al_strategy}\")\n",
        "    else:\n",
        "        draw_color = color_dictionary[(al_strategy,True)]\n",
        "\n",
        "    exp_results_list = experiment_results[(human_correct_strategy, auto_assign_strategy, b1, b2, b3, rounds, is_adaptive)]\n",
        "    with_pseudo_labeling_costs = get_avg_labeling_costs(exp_results_list, c_v, c_a, True, full_train_dataset)\n",
        "    without_pseudo_labeling_costs = get_avg_labeling_costs(exp_results_list, c_v, c_a, False, full_train_dataset)\n",
        "    average_acc, _ = get_avg_std_test_acc(exp_results_list)\n",
        "\n",
        "    plot_test_accuracies, labeling_efficiencies = produce_label_efficiency_data(without_pseudo_labeling_costs, average_acc, with_pseudo_labeling_costs, average_acc, label_eff_acc_granularity)\n",
        "\n",
        "    axes[0][1].plot(plot_test_accuracies, labeling_efficiencies, color=draw_color, marker='o')[0]\n",
        "\n",
        "# ===== ENTROPY =====\n",
        "al_strategy = \"entropy\"\n",
        "\n",
        "# Get CIFAR-10 results.\n",
        "results_directory = os.path.join(base_save_directory, dataset, al_strategy, str(seed_size))\n",
        "experiment_results = get_experiment_results(results_directory, dataset, al_strategy, seed_size, b_configs)\n",
        "\n",
        "# Get the average labeling costs across experiments along with the test accuracies. Plot them.\n",
        "for (human_correct_strategy, auto_assign_strategy, b1, b2, b3, rounds, is_adaptive) in experiment_results:\n",
        "\n",
        "    # Attempt to generate a color if there is not already one.\n",
        "    if (al_strategy,False) not in color_dictionary or (al_strategy,True) not in color_dictionary:\n",
        "        raise ValueError(F\"Add color for {al_strategy}\")\n",
        "    else:\n",
        "        no_suggestion_draw_color = color_dictionary[(al_strategy,False)]\n",
        "        suggestion_draw_color = color_dictionary[(al_strategy,True)]\n",
        "\n",
        "    # PLOT WITHOUT USING AL SUGGESTIONS\n",
        "    exp_results_list = experiment_results[(human_correct_strategy, auto_assign_strategy, b1, b2, b3, rounds, is_adaptive)]\n",
        "    labeling_costs = get_avg_labeling_costs(exp_results_list, c_v, c_a, False, full_train_dataset)\n",
        "    average_acc, std = get_avg_std_test_acc(exp_results_list)\n",
        "    lower_list = [(x-y) for (x,y) in zip(average_acc,std)]\n",
        "    upper_list = [(x+y) for (x,y) in zip(average_acc,std)]\n",
        "    line = axes[0][0].plot(labeling_costs, average_acc, color=no_suggestion_draw_color, marker='o', linestyle=\"--\")[0]\n",
        "    axes[0][0].fill_between(labeling_costs, lower_list, upper_list, alpha=shade_alpha, color=no_suggestion_draw_color)\n",
        "    \n",
        "    legend_line_dictionary[(al_strategy,False)] = line\n",
        "\n",
        "    # PLOT USING AL SUGGESTIONS\n",
        "    labeling_costs = get_avg_labeling_costs(exp_results_list, c_v, c_a, True, full_train_dataset)\n",
        "    average_acc, std = get_avg_std_test_acc(exp_results_list)\n",
        "    lower_list = [(x-y) for (x,y) in zip(average_acc,std)]\n",
        "    upper_list = [(x+y) for (x,y) in zip(average_acc,std)]\n",
        "    line = axes[0][0].plot(labeling_costs, average_acc, color=suggestion_draw_color, marker='o')[0]\n",
        "    axes[0][0].fill_between(labeling_costs, lower_list, upper_list, alpha=shade_alpha, color=suggestion_draw_color)\n",
        "\n",
        "    legend_line_dictionary[(al_strategy,True)] = line\n",
        "\n",
        "# Do the same for the labeling efficiency.\n",
        "\n",
        "for (human_correct_strategy, auto_assign_strategy, b1, b2, b3, rounds, is_adaptive) in experiment_results:\n",
        "\n",
        "    if (al_strategy,False) not in color_dictionary or (al_strategy,True) not in color_dictionary:\n",
        "        raise ValueError(F\"Add color for {al_strategy}\")\n",
        "    else:\n",
        "        draw_color = color_dictionary[(al_strategy,True)]\n",
        "\n",
        "    exp_results_list = experiment_results[(human_correct_strategy, auto_assign_strategy, b1, b2, b3, rounds, is_adaptive)]\n",
        "    with_pseudo_labeling_costs = get_avg_labeling_costs(exp_results_list, c_v, c_a, True, full_train_dataset)\n",
        "    without_pseudo_labeling_costs = get_avg_labeling_costs(exp_results_list, c_v, c_a, False, full_train_dataset)\n",
        "    average_acc, _ = get_avg_std_test_acc(exp_results_list)\n",
        "\n",
        "    plot_test_accuracies, labeling_efficiencies = produce_label_efficiency_data(without_pseudo_labeling_costs, average_acc, with_pseudo_labeling_costs, average_acc, label_eff_acc_granularity)\n",
        "\n",
        "    axes[0][1].plot(plot_test_accuracies, labeling_efficiencies, color=draw_color, marker='o')[0]\n",
        "\n",
        "# Create the legend by obtaining the list of labels\n",
        "label_list = []\n",
        "line_list = []\n",
        "for key in legend_line_dictionary:\n",
        "    line_list.append(legend_line_dictionary[key])\n",
        "    al_strategy = key[0].replace(\"_\", \"-\").capitalize()\n",
        "    pseudo_info = \"w/ Suggest\" if key[1] else \"w/o Suggest\"\n",
        "    label_list.append(r\"\\textsc{\" + F\"{al_strategy} {pseudo_info}\" + r\"}\")\n",
        "comparison_fig.legend(line_list, label_list, loc=\"upper center\", ncol=2, borderaxespad=3, fontsize=legend_font_size)"
      ],
      "metadata": {
        "id": "F5MgxP-66WI1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## EXPERIMENTS"
      ],
      "metadata": {
        "id": "RRUVXjhG50Tm"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### All Accuracies\n",
        "\n",
        "Layout: Each dataset has 2 plots per setting of $b_1,b_2,b_3$: one for test accuracy, one for labeling efficiency. This gives a total of 6 combinations for a total of 12 plots. We can do a 3 row, 4 col figure for now."
      ],
      "metadata": {
        "id": "fBrZ_XOW6f2N"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "nrows = 4\n",
        "ncols = 4\n",
        "\n",
        "figsize = (5.5 * ncols, 3.45 * nrows)\n",
        "comparison_fig, axes = plt.subplots(nrows=nrows, ncols=ncols, figsize=figsize, dpi=120, gridspec_kw = {'top':0.92,'wspace':0.35,'hspace':0.53})\n",
        "\n",
        "# Note: Figures are not to have titles. The caption serves this purpose.\n",
        "\n",
        "# Set error bar alpha\n",
        "shade_alpha = 0.25\n",
        "\n",
        "legend_line_dictionary = {}\n",
        "\n",
        "# Seed random color generator\n",
        "np.random.seed(42)\n",
        "\n",
        "# =========================================================================================================================================================\n",
        "#\n",
        "# ===================================================================== CIFAR-100 =========================================================================\n",
        "#\n",
        "# =========================================================================================================================================================\n",
        "\n",
        "# Note: b_configs is a list of tuples of the form (b_1, b_2, b_3, is_adaptive).\n",
        "dataset = \"cifar100\"\n",
        "seed_size = 10000\n",
        "b_configs = [(5000,0,0,False),\n",
        "             (500,2500,2000,False)]\n",
        "c_a = 4\n",
        "c_v = 1\n",
        "\n",
        "# Get CIFAR-10 dataset.\n",
        "full_train_dataset = datasets.CIFAR100(dataset_root_path, download=True, train=True)\n",
        "test_dataset = datasets.CIFAR100(dataset_root_path, download=True, train=False)\n",
        "nclasses = 100 # NUM CLASSES HERE\n",
        "    \n",
        "# Set the subplot title, y-axis, x-axis\n",
        "axes[0][0].set_title(r\"\\textbf{CIFAR-100}\", fontsize=plot_title_font_size)\n",
        "axes[0][0].set_xlabel(r\"\\textbf{Labeling Cost (\" + F\"{c_a},\" + F\"{c_v})\" + r\"}\", fontsize=axis_label_font_size)\n",
        "axes[0][0].set_ylabel(r\"\\textbf{Acc. (\" + F\"{500},\" + F\"{2500},\" + F\"{2000})\" + r\"}\", fontsize=axis_label_font_size)\n",
        "\n",
        "axes[0][1].set_title(r\"\\textbf{CIFAR-100}\", fontsize=plot_title_font_size)\n",
        "axes[0][1].set_xlabel(r\"\\textbf{Labeling Cost (\" + F\"{c_a},\" + F\"{c_v})\" + r\"}\", fontsize=axis_label_font_size)\n",
        "axes[0][1].set_ylabel(r\"\\textbf{Acc. (\" + F\"{500},\" + F\"{2500},\" + F\"{2000})\" + r\"}\", fontsize=axis_label_font_size)\n",
        "\n",
        "axes[0][2].set_title(r\"\\textbf{CIFAR-100}\", fontsize=plot_title_font_size)\n",
        "axes[0][2].set_xlabel(r\"\\textbf{Test Accuracy (\" + F\"{c_a},\" + F\"{c_v})\" + r\"}\", fontsize=axis_label_font_size)\n",
        "axes[0][2].set_ylabel(r\"\\textbf{Eff. (\" + F\"{500},\" + F\"{2500},\" + F\"{2000})\" + r\"}\", fontsize=axis_label_font_size)\n",
        "\n",
        "axes[0][3].set_title(r\"\\textbf{CIFAR-100}\", fontsize=plot_title_font_size)\n",
        "axes[0][3].set_xlabel(r\"\\textbf{Test Accuracy (\" + F\"{c_a},\" + F\"{c_v})\" + r\"}\", fontsize=axis_label_font_size)\n",
        "axes[0][3].set_ylabel(r\"\\textbf{Eff. (\" + F\"{500},\" + F\"{2500},\" + F\"{2000})\" + r\"}\", fontsize=axis_label_font_size)\n",
        "\n",
        "# ===== BADGE =====\n",
        "al_strategy = \"badge\"\n",
        "\n",
        "# Get CIFAR-10 results.\n",
        "results_directory = os.path.join(base_save_directory, dataset, al_strategy, str(seed_size))\n",
        "experiment_results = get_experiment_results(results_directory, dataset, al_strategy, seed_size, b_configs)\n",
        "suggestion_al_round_cutoff, no_suggestion_al_round_cutoff = determine_al_round_cutoff(experiment_results, full_train_dataset, c_v, c_a)\n",
        "\n",
        "# Get the average labeling costs across experiments along with the test accuracies. Plot them.\n",
        "for (human_correct_strategy, auto_assign_strategy, b1, b2, b3, rounds, is_adaptive) in experiment_results:\n",
        "\n",
        "    # Attempt to generate a color if there is not already one.\n",
        "    if b2 == 0 and b3 == 0:\n",
        "        if (al_strategy,False) not in color_dictionary or (al_strategy,True) not in color_dictionary:\n",
        "            raise ValueError(F\"Add color for {al_strategy}\")\n",
        "        else:\n",
        "            no_suggestion_draw_color = color_dictionary[(al_strategy,False)]\n",
        "            suggestion_draw_color = color_dictionary[(al_strategy,True)]\n",
        "\n",
        "            # Draw the suggest AL baseline\n",
        "            exp_results_list = experiment_results[(human_correct_strategy, auto_assign_strategy, b1, b2, b3, rounds, is_adaptive)]\n",
        "            print(F\"RUN COUNT {dataset} {al_strategy} {human_correct_strategy} {auto_assign_strategy} {b1} {b2} {b3} {seed_size} {rounds} {is_adaptive}: {len(exp_results_list)}\")\n",
        "            labeling_costs = get_avg_labeling_costs(exp_results_list, c_v, c_a, True, full_train_dataset)\n",
        "            average_acc, std = get_avg_std_test_acc(exp_results_list)\n",
        "            lower_list = [(x-y) for (x,y) in zip(average_acc,std)]\n",
        "            upper_list = [(x+y) for (x,y) in zip(average_acc,std)]\n",
        "            \n",
        "            line = axes[0][0].plot(labeling_costs[:suggestion_al_round_cutoff], average_acc[:suggestion_al_round_cutoff], color=suggestion_draw_color, marker='o')[0]\n",
        "            axes[0][0].fill_between(labeling_costs[:suggestion_al_round_cutoff], lower_list[:suggestion_al_round_cutoff], upper_list[:suggestion_al_round_cutoff], alpha=shade_alpha, color=suggestion_draw_color)\n",
        "\n",
        "            if (al_strategy, True) not in legend_line_dictionary:\n",
        "                legend_line_dictionary[(al_strategy, True)] = line\n",
        "\n",
        "            # Draw the no-suggest AL baseline\n",
        "            labeling_costs = get_avg_labeling_costs(exp_results_list, c_v, c_a, False, full_train_dataset)\n",
        "            average_acc, std = get_avg_std_test_acc(exp_results_list)\n",
        "            lower_list = [(x-y) for (x,y) in zip(average_acc,std)]\n",
        "            upper_list = [(x+y) for (x,y) in zip(average_acc,std)]\n",
        "            \n",
        "            line = axes[0][0].plot(labeling_costs[:no_suggestion_al_round_cutoff], average_acc[:no_suggestion_al_round_cutoff], color=no_suggestion_draw_color, marker='o')[0]\n",
        "            axes[0][0].fill_between(labeling_costs[:no_suggestion_al_round_cutoff], lower_list[:no_suggestion_al_round_cutoff], upper_list[:no_suggestion_al_round_cutoff], alpha=shade_alpha, color=no_suggestion_draw_color)\n",
        "\n",
        "            if (al_strategy, False) not in legend_line_dictionary:\n",
        "                legend_line_dictionary[(al_strategy, False)] = line\n",
        "\n",
        "            al_key = (human_correct_strategy, auto_assign_strategy, b1, b2, b3, rounds, is_adaptive)\n",
        "    else:\n",
        "        if (al_strategy, human_correct_strategy, auto_assign_strategy) not in color_dictionary:\n",
        "            raise ValueError(F\"Add color for {al_strategy}+{human_correct_strategy}+{auto_assign_strategy}\")\n",
        "        else:\n",
        "            draw_color = color_dictionary[(al_strategy, human_correct_strategy, auto_assign_strategy)]\n",
        "\n",
        "        exp_results_list = experiment_results[(human_correct_strategy, auto_assign_strategy, b1, b2, b3, rounds, is_adaptive)]\n",
        "        print(F\"RUN COUNT {dataset} {al_strategy} {human_correct_strategy} {auto_assign_strategy} {b1} {b2} {b3} {seed_size} {rounds} {is_adaptive}: {len(exp_results_list)}\")\n",
        "        labeling_costs = get_avg_labeling_costs(exp_results_list, c_v, c_a, True, full_train_dataset)\n",
        "        average_acc, std = get_avg_std_test_acc(exp_results_list)\n",
        "        lower_list = [(x-y) for (x,y) in zip(average_acc,std)]\n",
        "        upper_list = [(x+y) for (x,y) in zip(average_acc,std)]\n",
        "\n",
        "        line = axes[0][0].plot(labeling_costs, average_acc, color=draw_color, marker='o')[0]\n",
        "        axes[0][0].fill_between(labeling_costs, lower_list, upper_list, alpha=shade_alpha, color=draw_color)\n",
        "\n",
        "        if (al_strategy, human_correct_strategy, auto_assign_strategy) not in legend_line_dictionary:\n",
        "            legend_line_dictionary[(al_strategy, human_correct_strategy, auto_assign_strategy)] = line\n",
        "\n",
        "\n",
        "# Do the same for the labeling efficiency.\n",
        "al_exp_results_list = experiment_results[al_key]\n",
        "al_labeling_costs = get_avg_labeling_costs(al_exp_results_list, c_v, c_a, False, full_train_dataset)\n",
        "al_average_acc, _ = get_avg_std_test_acc(al_exp_results_list)\n",
        "\n",
        "for (human_correct_strategy, auto_assign_strategy, b1, b2, b3, rounds, is_adaptive) in experiment_results:\n",
        "\n",
        "    # If this is the AL key, plot the gain with suggestion over no suggestion.\n",
        "    if b2 == 0 and b3 == 0:\n",
        "        \n",
        "        suggestion_draw_color = color_dictionary[(al_strategy, True)]\n",
        "        exp_results_list = experiment_results[(human_correct_strategy, auto_assign_strategy, b1, b2, b3, rounds, is_adaptive)]\n",
        "        labeling_costs = get_avg_labeling_costs(exp_results_list, c_v, c_a, True, full_train_dataset)\n",
        "        average_acc, _ = get_avg_std_test_acc(exp_results_list)\n",
        "\n",
        "        plot_test_accuracies, labeling_efficiencies = produce_label_efficiency_data(al_labeling_costs[:suggestion_al_round_cutoff], al_average_acc[:suggestion_al_round_cutoff], labeling_costs[:suggestion_al_round_cutoff], average_acc[:suggestion_al_round_cutoff], label_eff_acc_granularity)\n",
        "\n",
        "        axes[0][2].plot(plot_test_accuracies, labeling_efficiencies, color=suggestion_draw_color, marker='o')[0]\n",
        "    else:\n",
        "\n",
        "        draw_color = color_dictionary[(al_strategy, human_correct_strategy, auto_assign_strategy)]\n",
        "        exp_results_list = experiment_results[(human_correct_strategy, auto_assign_strategy, b1, b2, b3, rounds, is_adaptive)]\n",
        "        labeling_costs = get_avg_labeling_costs(exp_results_list, c_v, c_a, True, full_train_dataset)\n",
        "        average_acc, _ = get_avg_std_test_acc(exp_results_list)\n",
        "\n",
        "        plot_test_accuracies, labeling_efficiencies = produce_label_efficiency_data(al_labeling_costs, al_average_acc, labeling_costs, average_acc, label_eff_acc_granularity)\n",
        "\n",
        "        axes[0][2].plot(plot_test_accuracies, labeling_efficiencies, color=draw_color, marker='o')[0]\n",
        "\n",
        "# ===== ENTROPY =====\n",
        "al_strategy = \"entropy\"\n",
        "del al_key\n",
        "\n",
        "# Get CIFAR-10 results.\n",
        "results_directory = os.path.join(base_save_directory, dataset, al_strategy, str(seed_size))\n",
        "experiment_results = get_experiment_results(results_directory, dataset, al_strategy, seed_size, b_configs)\n",
        "suggestion_al_round_cutoff, no_suggestion_al_round_cutoff = determine_al_round_cutoff(experiment_results, full_train_dataset, c_v, c_a)\n",
        "\n",
        "# Get the average labeling costs across experiments along with the test accuracies. Plot them.\n",
        "for (human_correct_strategy, auto_assign_strategy, b1, b2, b3, rounds, is_adaptive) in experiment_results:\n",
        "\n",
        "    # Attempt to generate a color if there is not already one.\n",
        "    if b2 == 0 and b3 == 0:\n",
        "        if (al_strategy,False) not in color_dictionary or (al_strategy,True) not in color_dictionary:\n",
        "            raise ValueError(F\"Add color for {al_strategy}\")\n",
        "        else:\n",
        "            no_suggestion_draw_color = color_dictionary[(al_strategy,False)]\n",
        "            suggestion_draw_color = color_dictionary[(al_strategy,True)]\n",
        "\n",
        "            # Draw the suggest AL baseline\n",
        "            exp_results_list = experiment_results[(human_correct_strategy, auto_assign_strategy, b1, b2, b3, rounds, is_adaptive)]\n",
        "            print(F\"RUN COUNT {dataset} {al_strategy} {human_correct_strategy} {auto_assign_strategy} {b1} {b2} {b3} {seed_size} {rounds} {is_adaptive}: {len(exp_results_list)}\")\n",
        "            labeling_costs = get_avg_labeling_costs(exp_results_list, c_v, c_a, True, full_train_dataset)\n",
        "            average_acc, std = get_avg_std_test_acc(exp_results_list)\n",
        "            lower_list = [(x-y) for (x,y) in zip(average_acc,std)]\n",
        "            upper_list = [(x+y) for (x,y) in zip(average_acc,std)]\n",
        "            \n",
        "            line = axes[0][1].plot(labeling_costs[:suggestion_al_round_cutoff], average_acc[:suggestion_al_round_cutoff], color=suggestion_draw_color, marker='o')[0]\n",
        "            axes[0][1].fill_between(labeling_costs[:suggestion_al_round_cutoff], lower_list[:suggestion_al_round_cutoff], upper_list[:suggestion_al_round_cutoff], alpha=shade_alpha, color=suggestion_draw_color)\n",
        "\n",
        "            if (al_strategy, True) not in legend_line_dictionary:\n",
        "                legend_line_dictionary[(al_strategy, True)] = line\n",
        "\n",
        "            # Draw the no-suggest AL baseline\n",
        "            labeling_costs = get_avg_labeling_costs(exp_results_list, c_v, c_a, False, full_train_dataset)\n",
        "            average_acc, std = get_avg_std_test_acc(exp_results_list)\n",
        "            lower_list = [(x-y) for (x,y) in zip(average_acc,std)]\n",
        "            upper_list = [(x+y) for (x,y) in zip(average_acc,std)]\n",
        "            \n",
        "            line = axes[0][1].plot(labeling_costs[:no_suggestion_al_round_cutoff], average_acc[:no_suggestion_al_round_cutoff], color=no_suggestion_draw_color, marker='o')[0]\n",
        "            axes[0][1].fill_between(labeling_costs[:no_suggestion_al_round_cutoff], lower_list[:no_suggestion_al_round_cutoff], upper_list[:no_suggestion_al_round_cutoff], alpha=shade_alpha, color=no_suggestion_draw_color)\n",
        "\n",
        "            if (al_strategy, False) not in legend_line_dictionary:\n",
        "                legend_line_dictionary[(al_strategy, False)] = line\n",
        "\n",
        "            al_key = (human_correct_strategy, auto_assign_strategy, b1, b2, b3, rounds, is_adaptive)\n",
        "    else:\n",
        "        if (al_strategy, human_correct_strategy, auto_assign_strategy) not in color_dictionary:\n",
        "            raise ValueError(F\"Add color for {al_strategy}+{human_correct_strategy}+{auto_assign_strategy}\")\n",
        "        else:\n",
        "            draw_color = color_dictionary[(al_strategy, human_correct_strategy, auto_assign_strategy)]\n",
        "\n",
        "        exp_results_list = experiment_results[(human_correct_strategy, auto_assign_strategy, b1, b2, b3, rounds, is_adaptive)]\n",
        "        print(F\"RUN COUNT {dataset} {al_strategy} {human_correct_strategy} {auto_assign_strategy} {b1} {b2} {b3} {seed_size} {rounds} {is_adaptive}: {len(exp_results_list)}\")\n",
        "        labeling_costs = get_avg_labeling_costs(exp_results_list, c_v, c_a, True, full_train_dataset)\n",
        "        average_acc, std = get_avg_std_test_acc(exp_results_list)\n",
        "        lower_list = [(x-y) for (x,y) in zip(average_acc,std)]\n",
        "        upper_list = [(x+y) for (x,y) in zip(average_acc,std)]\n",
        "\n",
        "        line = axes[0][1].plot(labeling_costs, average_acc, color=draw_color, marker='o')[0]\n",
        "        axes[0][1].fill_between(labeling_costs, lower_list, upper_list, alpha=shade_alpha, color=draw_color)\n",
        "\n",
        "        if (al_strategy, human_correct_strategy, auto_assign_strategy) not in legend_line_dictionary:\n",
        "            legend_line_dictionary[(al_strategy, human_correct_strategy, auto_assign_strategy)] = line\n",
        "\n",
        "\n",
        "# Do the same for the labeling efficiency.\n",
        "al_exp_results_list = experiment_results[al_key]\n",
        "al_labeling_costs = get_avg_labeling_costs(al_exp_results_list, c_v, c_a, False, full_train_dataset)\n",
        "al_average_acc, _ = get_avg_std_test_acc(al_exp_results_list)\n",
        "\n",
        "for (human_correct_strategy, auto_assign_strategy, b1, b2, b3, rounds, is_adaptive) in experiment_results:\n",
        "\n",
        "    # If this is the AL key, plot the gain with suggestion over no suggestion.\n",
        "    if b2 == 0 and b3 == 0:\n",
        "        \n",
        "        suggestion_draw_color = color_dictionary[(al_strategy, True)]\n",
        "        exp_results_list = experiment_results[(human_correct_strategy, auto_assign_strategy, b1, b2, b3, rounds, is_adaptive)]\n",
        "        labeling_costs = get_avg_labeling_costs(exp_results_list, c_v, c_a, True, full_train_dataset)\n",
        "        average_acc, _ = get_avg_std_test_acc(exp_results_list)\n",
        "\n",
        "        plot_test_accuracies, labeling_efficiencies = produce_label_efficiency_data(al_labeling_costs[:suggestion_al_round_cutoff], al_average_acc[:suggestion_al_round_cutoff], labeling_costs[:suggestion_al_round_cutoff], average_acc[:suggestion_al_round_cutoff], label_eff_acc_granularity)\n",
        "\n",
        "        axes[0][3].plot(plot_test_accuracies, labeling_efficiencies, color=suggestion_draw_color, marker='o')[0]\n",
        "    else:\n",
        "\n",
        "        draw_color = color_dictionary[(al_strategy, human_correct_strategy, auto_assign_strategy)]\n",
        "        exp_results_list = experiment_results[(human_correct_strategy, auto_assign_strategy, b1, b2, b3, rounds, is_adaptive)]\n",
        "        labeling_costs = get_avg_labeling_costs(exp_results_list, c_v, c_a, True, full_train_dataset)\n",
        "        average_acc, _ = get_avg_std_test_acc(exp_results_list)\n",
        "\n",
        "        plot_test_accuracies, labeling_efficiencies = produce_label_efficiency_data(al_labeling_costs, al_average_acc, labeling_costs, average_acc, label_eff_acc_granularity)\n",
        "\n",
        "        axes[0][3].plot(plot_test_accuracies, labeling_efficiencies, color=draw_color, marker='o')[0]\n",
        "\n",
        "# =========================================================================================================================================================\n",
        "#\n",
        "# ======================================================================= BIRDS ===========================================================================\n",
        "#\n",
        "# =========================================================================================================================================================\n",
        "\n",
        "# Note: b_configs is a list of tuples of the form (b_1, b_2, b_3, is_adaptive).\n",
        "dataset = \"birds\"\n",
        "seed_size = 600\n",
        "b_configs = [(600,0,0,False),\n",
        "             (60,400,140,False)]\n",
        "c_a = 4\n",
        "c_v = 1\n",
        "\n",
        "# Get CUB dataset.\n",
        "full_train_dataset = BirdsDataset(dataset_root_path, download=True, train=True)\n",
        "test_dataset = BirdsDataset(dataset_root_path, download=True, train=False)\n",
        "nclasses = 200\n",
        "\n",
        "# Set the subplot title, y-axis, x-axis\n",
        "axes[1][0].set_title(r\"\\textbf{CUB-200-2011}\", fontsize=plot_title_font_size)\n",
        "axes[1][0].set_xlabel(r\"\\textbf{Labeling Cost (\" + F\"{c_a},\" + F\"{c_v})\" + r\"}\", fontsize=axis_label_font_size)\n",
        "axes[1][0].set_ylabel(r\"\\textbf{Acc. (\" + F\"{60},\" + F\"{400},\" + F\"{140})\" + r\"}\", fontsize=axis_label_font_size)\n",
        "\n",
        "axes[1][1].set_title(r\"\\textbf{CUB-200-2011}\", fontsize=plot_title_font_size)\n",
        "axes[1][1].set_xlabel(r\"\\textbf{Labeling Cost (\" + F\"{c_a},\" + F\"{c_v})\" + r\"}\", fontsize=axis_label_font_size)\n",
        "axes[1][1].set_ylabel(r\"\\textbf{Acc. (\" + F\"{60},\" + F\"{400},\" + F\"{140})\" + r\"}\", fontsize=axis_label_font_size)\n",
        "\n",
        "axes[1][2].set_title(r\"\\textbf{CUB-200-2011}\", fontsize=plot_title_font_size)\n",
        "axes[1][2].set_xlabel(r\"\\textbf{Test Accuracy (\" + F\"{c_a},\" + F\"{c_v})\" + r\"}\", fontsize=axis_label_font_size)\n",
        "axes[1][2].set_ylabel(r\"\\textbf{Eff. (\" + F\"{60},\" + F\"{400},\" + F\"{140})\" + r\"}\", fontsize=axis_label_font_size)\n",
        "\n",
        "axes[1][3].set_title(r\"\\textbf{CUB-200-2011}\", fontsize=plot_title_font_size)\n",
        "axes[1][3].set_xlabel(r\"\\textbf{Test Accuracy (\" + F\"{c_a},\" + F\"{c_v})\" + r\"}\", fontsize=axis_label_font_size)\n",
        "axes[1][3].set_ylabel(r\"\\textbf{Eff. (\" + F\"{60},\" + F\"{400},\" + F\"{140})\" + r\"}\", fontsize=axis_label_font_size)\n",
        "\n",
        "# ===== BADGE =====\n",
        "al_strategy = \"badge\"\n",
        "\n",
        "# Get CIFAR-10 results.\n",
        "results_directory = os.path.join(base_save_directory, dataset, al_strategy, str(seed_size))\n",
        "experiment_results = get_experiment_results(results_directory, dataset, al_strategy, seed_size, b_configs)\n",
        "suggestion_al_round_cutoff, no_suggestion_al_round_cutoff = determine_al_round_cutoff(experiment_results, full_train_dataset, c_v, c_a)\n",
        "suggestion_al_round_cutoff -= 1\n",
        "no_suggestion_al_round_cutoff -= 1\n",
        "\n",
        "# Get the average labeling costs across experiments along with the test accuracies. Plot them.\n",
        "for (human_correct_strategy, auto_assign_strategy, b1, b2, b3, rounds, is_adaptive) in experiment_results:\n",
        "\n",
        "    # Attempt to generate a color if there is not already one.\n",
        "    if b2 == 0 and b3 == 0:\n",
        "        if (al_strategy,False) not in color_dictionary or (al_strategy,True) not in color_dictionary:\n",
        "            raise ValueError(F\"Add color for {al_strategy}\")\n",
        "        else:\n",
        "            no_suggestion_draw_color = color_dictionary[(al_strategy,False)]\n",
        "            suggestion_draw_color = color_dictionary[(al_strategy,True)]\n",
        "\n",
        "            # Draw the suggest AL baseline\n",
        "            exp_results_list = experiment_results[(human_correct_strategy, auto_assign_strategy, b1, b2, b3, rounds, is_adaptive)]\n",
        "            print(F\"RUN COUNT {dataset} {al_strategy} {human_correct_strategy} {auto_assign_strategy} {b1} {b2} {b3} {seed_size} {rounds} {is_adaptive}: {len(exp_results_list)}\")\n",
        "            labeling_costs = get_avg_labeling_costs(exp_results_list, c_v, c_a, True, full_train_dataset)\n",
        "            average_acc, std = get_avg_std_test_acc(exp_results_list)\n",
        "            lower_list = [(x-y) for (x,y) in zip(average_acc,std)]\n",
        "            upper_list = [(x+y) for (x,y) in zip(average_acc,std)]\n",
        "            \n",
        "            line = axes[1][0].plot(labeling_costs[:suggestion_al_round_cutoff], average_acc[:suggestion_al_round_cutoff], color=suggestion_draw_color, marker='o')[0]\n",
        "            axes[1][0].fill_between(labeling_costs[:suggestion_al_round_cutoff], lower_list[:suggestion_al_round_cutoff], upper_list[:suggestion_al_round_cutoff], alpha=shade_alpha, color=suggestion_draw_color)\n",
        "\n",
        "            if (al_strategy, True) not in legend_line_dictionary:\n",
        "                legend_line_dictionary[(al_strategy, True)] = line\n",
        "\n",
        "            # Draw the no-suggest AL baseline\n",
        "            labeling_costs = get_avg_labeling_costs(exp_results_list, c_v, c_a, False, full_train_dataset)\n",
        "            average_acc, std = get_avg_std_test_acc(exp_results_list)\n",
        "            lower_list = [(x-y) for (x,y) in zip(average_acc,std)]\n",
        "            upper_list = [(x+y) for (x,y) in zip(average_acc,std)]\n",
        "            \n",
        "            line = axes[1][0].plot(labeling_costs[:no_suggestion_al_round_cutoff], average_acc[:no_suggestion_al_round_cutoff], color=no_suggestion_draw_color, marker='o')[0]\n",
        "            axes[1][0].fill_between(labeling_costs[:no_suggestion_al_round_cutoff], lower_list[:no_suggestion_al_round_cutoff], upper_list[:no_suggestion_al_round_cutoff], alpha=shade_alpha, color=no_suggestion_draw_color)\n",
        "\n",
        "            if (al_strategy, False) not in legend_line_dictionary:\n",
        "                legend_line_dictionary[(al_strategy, False)] = line\n",
        "\n",
        "            al_key = (human_correct_strategy, auto_assign_strategy, b1, b2, b3, rounds, is_adaptive)\n",
        "    else:\n",
        "        if (al_strategy, human_correct_strategy, auto_assign_strategy) not in color_dictionary:\n",
        "            raise ValueError(F\"Add color for {al_strategy}+{human_correct_strategy}+{auto_assign_strategy}\")\n",
        "        else:\n",
        "            draw_color = color_dictionary[(al_strategy, human_correct_strategy, auto_assign_strategy)]\n",
        "\n",
        "        exp_results_list = experiment_results[(human_correct_strategy, auto_assign_strategy, b1, b2, b3, rounds, is_adaptive)]\n",
        "        print(F\"RUN COUNT {dataset} {al_strategy} {human_correct_strategy} {auto_assign_strategy} {b1} {b2} {b3} {seed_size} {rounds} {is_adaptive}: {len(exp_results_list)}\")\n",
        "        labeling_costs = get_avg_labeling_costs(exp_results_list, c_v, c_a, True, full_train_dataset)\n",
        "        average_acc, std = get_avg_std_test_acc(exp_results_list)\n",
        "        lower_list = [(x-y) for (x,y) in zip(average_acc,std)]\n",
        "        upper_list = [(x+y) for (x,y) in zip(average_acc,std)]\n",
        "\n",
        "        line = axes[1][0].plot(labeling_costs, average_acc, color=draw_color, marker='o')[0]\n",
        "        axes[1][0].fill_between(labeling_costs, lower_list, upper_list, alpha=shade_alpha, color=draw_color)\n",
        "\n",
        "        if (al_strategy, human_correct_strategy, auto_assign_strategy) not in legend_line_dictionary:\n",
        "            legend_line_dictionary[(al_strategy, human_correct_strategy, auto_assign_strategy)] = line\n",
        "\n",
        "\n",
        "# Do the same for the labeling efficiency.\n",
        "al_exp_results_list = experiment_results[al_key]\n",
        "al_labeling_costs = get_avg_labeling_costs(al_exp_results_list, c_v, c_a, False, full_train_dataset)\n",
        "al_average_acc, _ = get_avg_std_test_acc(al_exp_results_list)\n",
        "\n",
        "for (human_correct_strategy, auto_assign_strategy, b1, b2, b3, rounds, is_adaptive) in experiment_results:\n",
        "\n",
        "    # If this is the AL key, plot the gain with suggestion over no suggestion.\n",
        "    if b2 == 0 and b3 == 0:\n",
        "        \n",
        "        suggestion_draw_color = color_dictionary[(al_strategy, True)]\n",
        "        exp_results_list = experiment_results[(human_correct_strategy, auto_assign_strategy, b1, b2, b3, rounds, is_adaptive)]\n",
        "        labeling_costs = get_avg_labeling_costs(exp_results_list, c_v, c_a, True, full_train_dataset)\n",
        "        average_acc, _ = get_avg_std_test_acc(exp_results_list)\n",
        "\n",
        "        plot_test_accuracies, labeling_efficiencies = produce_label_efficiency_data(al_labeling_costs[:suggestion_al_round_cutoff], al_average_acc[:suggestion_al_round_cutoff], labeling_costs[:suggestion_al_round_cutoff], average_acc[:suggestion_al_round_cutoff], label_eff_acc_granularity)\n",
        "\n",
        "        axes[1][2].plot(plot_test_accuracies, labeling_efficiencies, color=suggestion_draw_color, marker='o')[0]\n",
        "    else:\n",
        "\n",
        "        draw_color = color_dictionary[(al_strategy, human_correct_strategy, auto_assign_strategy)]\n",
        "        exp_results_list = experiment_results[(human_correct_strategy, auto_assign_strategy, b1, b2, b3, rounds, is_adaptive)]\n",
        "        labeling_costs = get_avg_labeling_costs(exp_results_list, c_v, c_a, True, full_train_dataset)\n",
        "        average_acc, _ = get_avg_std_test_acc(exp_results_list)\n",
        "\n",
        "        plot_test_accuracies, labeling_efficiencies = produce_label_efficiency_data(al_labeling_costs, al_average_acc, labeling_costs, average_acc, label_eff_acc_granularity)\n",
        "\n",
        "        axes[1][2].plot(plot_test_accuracies, labeling_efficiencies, color=draw_color, marker='o')[0]\n",
        "\n",
        "# ===== ENTROPY =====\n",
        "al_strategy = \"entropy\"\n",
        "del al_key\n",
        "\n",
        "# Get CIFAR-10 results.\n",
        "results_directory = os.path.join(base_save_directory, dataset, al_strategy, str(seed_size))\n",
        "experiment_results = get_experiment_results(results_directory, dataset, al_strategy, seed_size, b_configs)\n",
        "suggestion_al_round_cutoff, no_suggestion_al_round_cutoff = determine_al_round_cutoff(experiment_results, full_train_dataset, c_v, c_a)\n",
        "\n",
        "# Get the average labeling costs across experiments along with the test accuracies. Plot them.\n",
        "for (human_correct_strategy, auto_assign_strategy, b1, b2, b3, rounds, is_adaptive) in experiment_results:\n",
        "\n",
        "    # Attempt to generate a color if there is not already one.\n",
        "    if b2 == 0 and b3 == 0:\n",
        "        if (al_strategy,False) not in color_dictionary or (al_strategy,True) not in color_dictionary:\n",
        "            raise ValueError(F\"Add color for {al_strategy}\")\n",
        "        else:\n",
        "            no_suggestion_draw_color = color_dictionary[(al_strategy,False)]\n",
        "            suggestion_draw_color = color_dictionary[(al_strategy,True)]\n",
        "\n",
        "            # Draw the suggest AL baseline\n",
        "            exp_results_list = experiment_results[(human_correct_strategy, auto_assign_strategy, b1, b2, b3, rounds, is_adaptive)]\n",
        "            print(F\"RUN COUNT {dataset} {al_strategy} {human_correct_strategy} {auto_assign_strategy} {b1} {b2} {b3} {seed_size} {rounds} {is_adaptive}: {len(exp_results_list)}\")\n",
        "            labeling_costs = get_avg_labeling_costs(exp_results_list, c_v, c_a, True, full_train_dataset)\n",
        "            average_acc, std = get_avg_std_test_acc(exp_results_list)\n",
        "            lower_list = [(x-y) for (x,y) in zip(average_acc,std)]\n",
        "            upper_list = [(x+y) for (x,y) in zip(average_acc,std)]\n",
        "            \n",
        "            line = axes[1][1].plot(labeling_costs[:suggestion_al_round_cutoff], average_acc[:suggestion_al_round_cutoff], color=suggestion_draw_color, marker='o')[0]\n",
        "            axes[1][1].fill_between(labeling_costs[:suggestion_al_round_cutoff], lower_list[:suggestion_al_round_cutoff], upper_list[:suggestion_al_round_cutoff], alpha=shade_alpha, color=suggestion_draw_color)\n",
        "\n",
        "            if (al_strategy, True) not in legend_line_dictionary:\n",
        "                legend_line_dictionary[(al_strategy, True)] = line\n",
        "\n",
        "            # Draw the no-suggest AL baseline\n",
        "            labeling_costs = get_avg_labeling_costs(exp_results_list, c_v, c_a, False, full_train_dataset)\n",
        "            average_acc, std = get_avg_std_test_acc(exp_results_list)\n",
        "            lower_list = [(x-y) for (x,y) in zip(average_acc,std)]\n",
        "            upper_list = [(x+y) for (x,y) in zip(average_acc,std)]\n",
        "            \n",
        "            line = axes[1][1].plot(labeling_costs[:no_suggestion_al_round_cutoff], average_acc[:no_suggestion_al_round_cutoff], color=no_suggestion_draw_color, marker='o')[0]\n",
        "            axes[1][1].fill_between(labeling_costs[:no_suggestion_al_round_cutoff], lower_list[:no_suggestion_al_round_cutoff], upper_list[:no_suggestion_al_round_cutoff], alpha=shade_alpha, color=no_suggestion_draw_color)\n",
        "\n",
        "            if (al_strategy, False) not in legend_line_dictionary:\n",
        "                legend_line_dictionary[(al_strategy, False)] = line\n",
        "\n",
        "            al_key = (human_correct_strategy, auto_assign_strategy, b1, b2, b3, rounds, is_adaptive)\n",
        "    else:\n",
        "        if (al_strategy, human_correct_strategy, auto_assign_strategy) not in color_dictionary:\n",
        "            raise ValueError(F\"Add color for {al_strategy}+{human_correct_strategy}+{auto_assign_strategy}\")\n",
        "        else:\n",
        "            draw_color = color_dictionary[(al_strategy, human_correct_strategy, auto_assign_strategy)]\n",
        "\n",
        "        exp_results_list = experiment_results[(human_correct_strategy, auto_assign_strategy, b1, b2, b3, rounds, is_adaptive)]\n",
        "        print(F\"RUN COUNT {dataset} {al_strategy} {human_correct_strategy} {auto_assign_strategy} {b1} {b2} {b3} {seed_size} {rounds} {is_adaptive}: {len(exp_results_list)}\")\n",
        "        labeling_costs = get_avg_labeling_costs(exp_results_list, c_v, c_a, True, full_train_dataset)\n",
        "        average_acc, std = get_avg_std_test_acc(exp_results_list)\n",
        "        lower_list = [(x-y) for (x,y) in zip(average_acc,std)]\n",
        "        upper_list = [(x+y) for (x,y) in zip(average_acc,std)]\n",
        "\n",
        "        line = axes[1][1].plot(labeling_costs, average_acc, color=draw_color, marker='o')[0]\n",
        "        axes[1][1].fill_between(labeling_costs, lower_list, upper_list, alpha=shade_alpha, color=draw_color)\n",
        "\n",
        "        if (al_strategy, human_correct_strategy, auto_assign_strategy) not in legend_line_dictionary:\n",
        "            legend_line_dictionary[(al_strategy, human_correct_strategy, auto_assign_strategy)] = line\n",
        "\n",
        "\n",
        "# Do the same for the labeling efficiency.\n",
        "al_exp_results_list = experiment_results[al_key]\n",
        "al_labeling_costs = get_avg_labeling_costs(al_exp_results_list, c_v, c_a, False, full_train_dataset)\n",
        "al_average_acc, _ = get_avg_std_test_acc(al_exp_results_list)\n",
        "\n",
        "for (human_correct_strategy, auto_assign_strategy, b1, b2, b3, rounds, is_adaptive) in experiment_results:\n",
        "\n",
        "    # If this is the AL key, plot the gain with suggestion over no suggestion.\n",
        "    if b2 == 0 and b3 == 0:\n",
        "        \n",
        "        suggestion_draw_color = color_dictionary[(al_strategy, True)]\n",
        "        exp_results_list = experiment_results[(human_correct_strategy, auto_assign_strategy, b1, b2, b3, rounds, is_adaptive)]\n",
        "        labeling_costs = get_avg_labeling_costs(exp_results_list, c_v, c_a, True, full_train_dataset)\n",
        "        average_acc, _ = get_avg_std_test_acc(exp_results_list)\n",
        "\n",
        "        plot_test_accuracies, labeling_efficiencies = produce_label_efficiency_data(al_labeling_costs[:suggestion_al_round_cutoff], al_average_acc[:suggestion_al_round_cutoff], labeling_costs[:suggestion_al_round_cutoff], average_acc[:suggestion_al_round_cutoff], label_eff_acc_granularity)\n",
        "\n",
        "        axes[1][3].plot(plot_test_accuracies, labeling_efficiencies, color=suggestion_draw_color, marker='o')[0]\n",
        "    else:\n",
        "\n",
        "        draw_color = color_dictionary[(al_strategy, human_correct_strategy, auto_assign_strategy)]\n",
        "        exp_results_list = experiment_results[(human_correct_strategy, auto_assign_strategy, b1, b2, b3, rounds, is_adaptive)]\n",
        "        labeling_costs = get_avg_labeling_costs(exp_results_list, c_v, c_a, True, full_train_dataset)\n",
        "        average_acc, _ = get_avg_std_test_acc(exp_results_list)\n",
        "\n",
        "        plot_test_accuracies, labeling_efficiencies = produce_label_efficiency_data(al_labeling_costs, al_average_acc, labeling_costs, average_acc, label_eff_acc_granularity)\n",
        "\n",
        "        axes[1][3].plot(plot_test_accuracies, labeling_efficiencies, color=draw_color, marker='o')[0]\n",
        "\n",
        "# =========================================================================================================================================================\n",
        "#\n",
        "# ==================================================================== BIRDS PART 2 =======================================================================\n",
        "#\n",
        "# =========================================================================================================================================================\n",
        "\n",
        "# Note: b_configs is a list of tuples of the form (b_1, b_2, b_3, is_adaptive).\n",
        "dataset = \"birds\"\n",
        "seed_size = 600\n",
        "b_configs = [(600,0,0,False),\n",
        "             (60,300,240,False)]\n",
        "\n",
        "c_a = 4\n",
        "c_v = 1\n",
        "\n",
        "# Get CUB dataset.\n",
        "full_train_dataset = BirdsDataset(dataset_root_path, download=True, train=True)\n",
        "test_dataset = BirdsDataset(dataset_root_path, download=True, train=False)\n",
        "nclasses = 200\n",
        "\n",
        "# Set the subplot title, y-axis, x-axis\n",
        "axes[2][0].set_title(r\"\\textbf{CUB-200-2011}\", fontsize=plot_title_font_size)\n",
        "axes[2][0].set_xlabel(r\"\\textbf{Labeling Cost (\" + F\"{c_a},\" + F\"{c_v})\" + r\"}\", fontsize=axis_label_font_size)\n",
        "axes[2][0].set_ylabel(r\"\\textbf{Acc. (\" + F\"{60},\" + F\"{300},\" + F\"{240})\" + r\"}\", fontsize=axis_label_font_size)\n",
        "\n",
        "axes[2][1].set_title(r\"\\textbf{CUB-200-2011}\", fontsize=plot_title_font_size)\n",
        "axes[2][1].set_xlabel(r\"\\textbf{Labeling Cost (\" + F\"{c_a},\" + F\"{c_v})\" + r\"}\", fontsize=axis_label_font_size)\n",
        "axes[2][1].set_ylabel(r\"\\textbf{Acc. (\" + F\"{60},\" + F\"{300},\" + F\"{240})\" + r\"}\", fontsize=axis_label_font_size)\n",
        "\n",
        "axes[2][2].set_title(r\"\\textbf{CUB-200-2011}\", fontsize=plot_title_font_size)\n",
        "axes[2][2].set_xlabel(r\"\\textbf{Test Accuracy (\" + F\"{c_a},\" + F\"{c_v})\" + r\"}\", fontsize=axis_label_font_size)\n",
        "axes[2][2].set_ylabel(r\"\\textbf{Eff. (\" + F\"{60},\" + F\"{300},\" + F\"{240})\" + r\"}\", fontsize=axis_label_font_size)\n",
        "\n",
        "axes[2][3].set_title(r\"\\textbf{CUB-200-2011}\", fontsize=plot_title_font_size)\n",
        "axes[2][3].set_xlabel(r\"\\textbf{Test Accuracy (\" + F\"{c_a},\" + F\"{c_v})\" + r\"}\", fontsize=axis_label_font_size)\n",
        "axes[2][3].set_ylabel(r\"\\textbf{Eff. (\" + F\"{60},\" + F\"{300},\" + F\"{240})\" + r\"}\", fontsize=axis_label_font_size)\n",
        "\n",
        "# ===== BADGE =====\n",
        "al_strategy = \"badge\"\n",
        "\n",
        "# Get CIFAR-10 results.\n",
        "results_directory = os.path.join(base_save_directory, dataset, al_strategy, str(seed_size))\n",
        "experiment_results = get_experiment_results(results_directory, dataset, al_strategy, seed_size, b_configs)\n",
        "suggestion_al_round_cutoff, no_suggestion_al_round_cutoff = determine_al_round_cutoff(experiment_results, full_train_dataset, c_v, c_a)\n",
        "suggestion_al_round_cutoff -= 1\n",
        "no_suggestion_al_round_cutoff -= 1\n",
        "\n",
        "# Get the average labeling costs across experiments along with the test accuracies. Plot them.\n",
        "for (human_correct_strategy, auto_assign_strategy, b1, b2, b3, rounds, is_adaptive) in experiment_results:\n",
        "\n",
        "    # Attempt to generate a color if there is not already one.\n",
        "    if b2 == 0 and b3 == 0:\n",
        "        if (al_strategy,False) not in color_dictionary or (al_strategy,True) not in color_dictionary:\n",
        "            raise ValueError(F\"Add color for {al_strategy}\")\n",
        "        else:\n",
        "            no_suggestion_draw_color = color_dictionary[(al_strategy,False)]\n",
        "            suggestion_draw_color = color_dictionary[(al_strategy,True)]\n",
        "\n",
        "            # Draw the suggest AL baseline\n",
        "            exp_results_list = experiment_results[(human_correct_strategy, auto_assign_strategy, b1, b2, b3, rounds, is_adaptive)]\n",
        "            print(F\"RUN COUNT {dataset} {al_strategy} {human_correct_strategy} {auto_assign_strategy} {b1} {b2} {b3} {seed_size} {rounds} {is_adaptive}: {len(exp_results_list)}\")\n",
        "            labeling_costs = get_avg_labeling_costs(exp_results_list, c_v, c_a, True, full_train_dataset)\n",
        "            average_acc, std = get_avg_std_test_acc(exp_results_list)\n",
        "            lower_list = [(x-y) for (x,y) in zip(average_acc,std)]\n",
        "            upper_list = [(x+y) for (x,y) in zip(average_acc,std)]\n",
        "            \n",
        "            line = axes[2][0].plot(labeling_costs[:suggestion_al_round_cutoff], average_acc[:suggestion_al_round_cutoff], color=suggestion_draw_color, marker='o')[0]\n",
        "            axes[2][0].fill_between(labeling_costs[:suggestion_al_round_cutoff], lower_list[:suggestion_al_round_cutoff], upper_list[:suggestion_al_round_cutoff], alpha=shade_alpha, color=suggestion_draw_color)\n",
        "\n",
        "            if (al_strategy, True) not in legend_line_dictionary:\n",
        "                legend_line_dictionary[(al_strategy, True)] = line\n",
        "\n",
        "            # Draw the no-suggest AL baseline\n",
        "            labeling_costs = get_avg_labeling_costs(exp_results_list, c_v, c_a, False, full_train_dataset)\n",
        "            average_acc, std = get_avg_std_test_acc(exp_results_list)\n",
        "            lower_list = [(x-y) for (x,y) in zip(average_acc,std)]\n",
        "            upper_list = [(x+y) for (x,y) in zip(average_acc,std)]\n",
        "            \n",
        "            line = axes[2][0].plot(labeling_costs[:no_suggestion_al_round_cutoff], average_acc[:no_suggestion_al_round_cutoff], color=no_suggestion_draw_color, marker='o')[0]\n",
        "            axes[2][0].fill_between(labeling_costs[:no_suggestion_al_round_cutoff], lower_list[:no_suggestion_al_round_cutoff], upper_list[:no_suggestion_al_round_cutoff], alpha=shade_alpha, color=no_suggestion_draw_color)\n",
        "\n",
        "            if (al_strategy, False) not in legend_line_dictionary:\n",
        "                legend_line_dictionary[(al_strategy, False)] = line\n",
        "\n",
        "            al_key = (human_correct_strategy, auto_assign_strategy, b1, b2, b3, rounds, is_adaptive)\n",
        "    else:\n",
        "        if (al_strategy, human_correct_strategy, auto_assign_strategy) not in color_dictionary:\n",
        "            raise ValueError(F\"Add color for {al_strategy}+{human_correct_strategy}+{auto_assign_strategy}\")\n",
        "        else:\n",
        "            draw_color = color_dictionary[(al_strategy, human_correct_strategy, auto_assign_strategy)]\n",
        "\n",
        "        exp_results_list = experiment_results[(human_correct_strategy, auto_assign_strategy, b1, b2, b3, rounds, is_adaptive)]\n",
        "        print(F\"RUN COUNT {dataset} {al_strategy} {human_correct_strategy} {auto_assign_strategy} {b1} {b2} {b3} {seed_size} {rounds} {is_adaptive}: {len(exp_results_list)}\")\n",
        "        labeling_costs = get_avg_labeling_costs(exp_results_list, c_v, c_a, True, full_train_dataset)\n",
        "        average_acc, std = get_avg_std_test_acc(exp_results_list)\n",
        "        lower_list = [(x-y) for (x,y) in zip(average_acc,std)]\n",
        "        upper_list = [(x+y) for (x,y) in zip(average_acc,std)]\n",
        "\n",
        "        line = axes[2][0].plot(labeling_costs, average_acc, color=draw_color, marker='o')[0]\n",
        "        axes[2][0].fill_between(labeling_costs, lower_list, upper_list, alpha=shade_alpha, color=draw_color)\n",
        "\n",
        "        if (al_strategy, human_correct_strategy, auto_assign_strategy) not in legend_line_dictionary:\n",
        "            legend_line_dictionary[(al_strategy, human_correct_strategy, auto_assign_strategy)] = line\n",
        "\n",
        "\n",
        "# Do the same for the labeling efficiency.\n",
        "al_exp_results_list = experiment_results[al_key]\n",
        "al_labeling_costs = get_avg_labeling_costs(al_exp_results_list, c_v, c_a, False, full_train_dataset)\n",
        "al_average_acc, _ = get_avg_std_test_acc(al_exp_results_list)\n",
        "\n",
        "for (human_correct_strategy, auto_assign_strategy, b1, b2, b3, rounds, is_adaptive) in experiment_results:\n",
        "\n",
        "    # If this is the AL key, plot the gain with suggestion over no suggestion.\n",
        "    if b2 == 0 and b3 == 0:\n",
        "        \n",
        "        suggestion_draw_color = color_dictionary[(al_strategy, True)]\n",
        "        exp_results_list = experiment_results[(human_correct_strategy, auto_assign_strategy, b1, b2, b3, rounds, is_adaptive)]\n",
        "        labeling_costs = get_avg_labeling_costs(exp_results_list, c_v, c_a, True, full_train_dataset)\n",
        "        average_acc, _ = get_avg_std_test_acc(exp_results_list)\n",
        "\n",
        "        plot_test_accuracies, labeling_efficiencies = produce_label_efficiency_data(al_labeling_costs[:suggestion_al_round_cutoff], al_average_acc[:suggestion_al_round_cutoff], labeling_costs[:suggestion_al_round_cutoff], average_acc[:suggestion_al_round_cutoff], label_eff_acc_granularity)\n",
        "\n",
        "        axes[2][2].plot(plot_test_accuracies, labeling_efficiencies, color=suggestion_draw_color, marker='o')[0]\n",
        "    else:\n",
        "\n",
        "        draw_color = color_dictionary[(al_strategy, human_correct_strategy, auto_assign_strategy)]\n",
        "        exp_results_list = experiment_results[(human_correct_strategy, auto_assign_strategy, b1, b2, b3, rounds, is_adaptive)]\n",
        "        labeling_costs = get_avg_labeling_costs(exp_results_list, c_v, c_a, True, full_train_dataset)\n",
        "        average_acc, _ = get_avg_std_test_acc(exp_results_list)\n",
        "\n",
        "        plot_test_accuracies, labeling_efficiencies = produce_label_efficiency_data(al_labeling_costs, al_average_acc, labeling_costs, average_acc, label_eff_acc_granularity)\n",
        "\n",
        "        axes[2][2].plot(plot_test_accuracies, labeling_efficiencies, color=draw_color, marker='o')[0]\n",
        "\n",
        "# ===== ENTROPY =====\n",
        "al_strategy = \"entropy\"\n",
        "del al_key\n",
        "\n",
        "# Get CIFAR-10 results.\n",
        "results_directory = os.path.join(base_save_directory, dataset, al_strategy, str(seed_size))\n",
        "experiment_results = get_experiment_results(results_directory, dataset, al_strategy, seed_size, b_configs)\n",
        "suggestion_al_round_cutoff, no_suggestion_al_round_cutoff = determine_al_round_cutoff(experiment_results, full_train_dataset, c_v, c_a)\n",
        "\n",
        "# Get the average labeling costs across experiments along with the test accuracies. Plot them.\n",
        "for (human_correct_strategy, auto_assign_strategy, b1, b2, b3, rounds, is_adaptive) in experiment_results:\n",
        "\n",
        "    # Attempt to generate a color if there is not already one.\n",
        "    if b2 == 0 and b3 == 0:\n",
        "        if (al_strategy,False) not in color_dictionary or (al_strategy,True) not in color_dictionary:\n",
        "            raise ValueError(F\"Add color for {al_strategy}\")\n",
        "        else:\n",
        "            no_suggestion_draw_color = color_dictionary[(al_strategy,False)]\n",
        "            suggestion_draw_color = color_dictionary[(al_strategy,True)]\n",
        "\n",
        "            # Draw the suggest AL baseline\n",
        "            exp_results_list = experiment_results[(human_correct_strategy, auto_assign_strategy, b1, b2, b3, rounds, is_adaptive)]\n",
        "            print(F\"RUN COUNT {dataset} {al_strategy} {human_correct_strategy} {auto_assign_strategy} {b1} {b2} {b3} {seed_size} {rounds} {is_adaptive}: {len(exp_results_list)}\")\n",
        "            labeling_costs = get_avg_labeling_costs(exp_results_list, c_v, c_a, True, full_train_dataset)\n",
        "            average_acc, std = get_avg_std_test_acc(exp_results_list)\n",
        "            lower_list = [(x-y) for (x,y) in zip(average_acc,std)]\n",
        "            upper_list = [(x+y) for (x,y) in zip(average_acc,std)]\n",
        "            \n",
        "            line = axes[2][1].plot(labeling_costs[:suggestion_al_round_cutoff], average_acc[:suggestion_al_round_cutoff], color=suggestion_draw_color, marker='o')[0]\n",
        "            axes[2][1].fill_between(labeling_costs[:suggestion_al_round_cutoff], lower_list[:suggestion_al_round_cutoff], upper_list[:suggestion_al_round_cutoff], alpha=shade_alpha, color=suggestion_draw_color)\n",
        "\n",
        "            if (al_strategy, True) not in legend_line_dictionary:\n",
        "                legend_line_dictionary[(al_strategy, True)] = line\n",
        "\n",
        "            # Draw the no-suggest AL baseline\n",
        "            labeling_costs = get_avg_labeling_costs(exp_results_list, c_v, c_a, False, full_train_dataset)\n",
        "            average_acc, std = get_avg_std_test_acc(exp_results_list)\n",
        "            lower_list = [(x-y) for (x,y) in zip(average_acc,std)]\n",
        "            upper_list = [(x+y) for (x,y) in zip(average_acc,std)]\n",
        "            \n",
        "            line = axes[2][1].plot(labeling_costs[:no_suggestion_al_round_cutoff], average_acc[:no_suggestion_al_round_cutoff], color=no_suggestion_draw_color, marker='o')[0]\n",
        "            axes[2][1].fill_between(labeling_costs[:no_suggestion_al_round_cutoff], lower_list[:no_suggestion_al_round_cutoff], upper_list[:no_suggestion_al_round_cutoff], alpha=shade_alpha, color=no_suggestion_draw_color)\n",
        "\n",
        "            if (al_strategy, False) not in legend_line_dictionary:\n",
        "                legend_line_dictionary[(al_strategy, False)] = line\n",
        "\n",
        "            al_key = (human_correct_strategy, auto_assign_strategy, b1, b2, b3, rounds, is_adaptive)\n",
        "    else:\n",
        "        if (al_strategy, human_correct_strategy, auto_assign_strategy) not in color_dictionary:\n",
        "            raise ValueError(F\"Add color for {al_strategy}+{human_correct_strategy}+{auto_assign_strategy}\")\n",
        "        else:\n",
        "            draw_color = color_dictionary[(al_strategy, human_correct_strategy, auto_assign_strategy)]\n",
        "\n",
        "        exp_results_list = experiment_results[(human_correct_strategy, auto_assign_strategy, b1, b2, b3, rounds, is_adaptive)]\n",
        "        print(F\"RUN COUNT {dataset} {al_strategy} {human_correct_strategy} {auto_assign_strategy} {b1} {b2} {b3} {seed_size} {rounds} {is_adaptive}: {len(exp_results_list)}\")\n",
        "        labeling_costs = get_avg_labeling_costs(exp_results_list, c_v, c_a, True, full_train_dataset)\n",
        "        average_acc, std = get_avg_std_test_acc(exp_results_list)\n",
        "        lower_list = [(x-y) for (x,y) in zip(average_acc,std)]\n",
        "        upper_list = [(x+y) for (x,y) in zip(average_acc,std)]\n",
        "\n",
        "        line = axes[2][1].plot(labeling_costs, average_acc, color=draw_color, marker='o')[0]\n",
        "        axes[2][1].fill_between(labeling_costs, lower_list, upper_list, alpha=shade_alpha, color=draw_color)\n",
        "\n",
        "        if (al_strategy, human_correct_strategy, auto_assign_strategy) not in legend_line_dictionary:\n",
        "            legend_line_dictionary[(al_strategy, human_correct_strategy, auto_assign_strategy)] = line\n",
        "\n",
        "\n",
        "# Do the same for the labeling efficiency.\n",
        "al_exp_results_list = experiment_results[al_key]\n",
        "al_labeling_costs = get_avg_labeling_costs(al_exp_results_list, c_v, c_a, False, full_train_dataset)\n",
        "al_average_acc, _ = get_avg_std_test_acc(al_exp_results_list)\n",
        "\n",
        "for (human_correct_strategy, auto_assign_strategy, b1, b2, b3, rounds, is_adaptive) in experiment_results:\n",
        "\n",
        "    # If this is the AL key, plot the gain with suggestion over no suggestion.\n",
        "    if b2 == 0 and b3 == 0:\n",
        "        \n",
        "        suggestion_draw_color = color_dictionary[(al_strategy, True)]\n",
        "        exp_results_list = experiment_results[(human_correct_strategy, auto_assign_strategy, b1, b2, b3, rounds, is_adaptive)]\n",
        "        labeling_costs = get_avg_labeling_costs(exp_results_list, c_v, c_a, True, full_train_dataset)\n",
        "        average_acc, _ = get_avg_std_test_acc(exp_results_list)\n",
        "\n",
        "        plot_test_accuracies, labeling_efficiencies = produce_label_efficiency_data(al_labeling_costs[:suggestion_al_round_cutoff], al_average_acc[:suggestion_al_round_cutoff], labeling_costs[:suggestion_al_round_cutoff], average_acc[:suggestion_al_round_cutoff], label_eff_acc_granularity)\n",
        "\n",
        "        axes[2][3].plot(plot_test_accuracies, labeling_efficiencies, color=suggestion_draw_color, marker='o')[0]\n",
        "    else:\n",
        "\n",
        "        draw_color = color_dictionary[(al_strategy, human_correct_strategy, auto_assign_strategy)]\n",
        "        exp_results_list = experiment_results[(human_correct_strategy, auto_assign_strategy, b1, b2, b3, rounds, is_adaptive)]\n",
        "        labeling_costs = get_avg_labeling_costs(exp_results_list, c_v, c_a, True, full_train_dataset)\n",
        "        average_acc, _ = get_avg_std_test_acc(exp_results_list)\n",
        "\n",
        "        plot_test_accuracies, labeling_efficiencies = produce_label_efficiency_data(al_labeling_costs, al_average_acc, labeling_costs, average_acc, label_eff_acc_granularity)\n",
        "\n",
        "        axes[2][3].plot(plot_test_accuracies, labeling_efficiencies, color=draw_color, marker='o')[0]\n",
        "\n",
        "# =========================================================================================================================================================\n",
        "#\n",
        "# ======================================================================== DOGS ===========================================================================\n",
        "#\n",
        "# =========================================================================================================================================================\n",
        "\n",
        "# Note: b_configs is a list of tuples of the form (b_1, b_2, b_3, is_adaptive).\n",
        "dataset = \"dogs\"\n",
        "seed_size = 500\n",
        "b_configs = [(1000,0,0,False),\n",
        "             (100,600,300,False)]\n",
        "\n",
        "c_a = 4\n",
        "c_v = 1\n",
        "\n",
        "# Get Dogs dataset.\n",
        "full_train_dataset = DogsDataset(dataset_root_path, download=True, train=True)\n",
        "test_dataset = DogsDataset(dataset_root_path, download=True, train=False)\n",
        "nclasses = 120\n",
        "\n",
        "# Set the subplot title, y-axis, x-axis\n",
        "axes[3][0].set_title(r\"\\textbf{Stanford Dogs}\", fontsize=plot_title_font_size)\n",
        "axes[3][0].set_xlabel(r\"\\textbf{Labeling Cost (\" + F\"{c_a},\" + F\"{c_v})\" + r\"}\", fontsize=axis_label_font_size)\n",
        "axes[3][0].set_ylabel(r\"\\textbf{Acc. (\" + F\"{100},\" + F\"{600},\" + F\"{300})\" + r\"}\", fontsize=axis_label_font_size)\n",
        "\n",
        "axes[3][1].set_title(r\"\\textbf{Stanford Dogs}\", fontsize=plot_title_font_size)\n",
        "axes[3][1].set_xlabel(r\"\\textbf{Labeling Cost (\" + F\"{c_a},\" + F\"{c_v})\" + r\"}\", fontsize=axis_label_font_size)\n",
        "axes[3][1].set_ylabel(r\"\\textbf{Acc. (\" + F\"{100},\" + F\"{600},\" + F\"{300})\" + r\"}\", fontsize=axis_label_font_size)\n",
        "\n",
        "axes[3][2].set_title(r\"\\textbf{Stanford Dogs}\", fontsize=plot_title_font_size)\n",
        "axes[3][2].set_xlabel(r\"\\textbf{Test Accuracy (\" + F\"{c_a},\" + F\"{c_v})\" + r\"}\", fontsize=axis_label_font_size)\n",
        "axes[3][2].set_ylabel(r\"\\textbf{Eff. (\" + F\"{100},\" + F\"{600},\" + F\"{300})\" + r\"}\", fontsize=axis_label_font_size)\n",
        "\n",
        "axes[3][3].set_title(r\"\\textbf{Stanford Dogs}\", fontsize=plot_title_font_size)\n",
        "axes[3][3].set_xlabel(r\"\\textbf{Test Accuracy (\" + F\"{c_a},\" + F\"{c_v})\" + r\"}\", fontsize=axis_label_font_size)\n",
        "axes[3][3].set_ylabel(r\"\\textbf{Eff. (\" + F\"{100},\" + F\"{600},\" + F\"{300})\" + r\"}\", fontsize=axis_label_font_size)\n",
        "\n",
        "# ===== BADGE =====\n",
        "al_strategy = \"badge\"\n",
        "\n",
        "# Get CIFAR-10 results.\n",
        "results_directory = os.path.join(base_save_directory, dataset, al_strategy, str(seed_size))\n",
        "experiment_results = get_experiment_results(results_directory, dataset, al_strategy, seed_size, b_configs)\n",
        "suggestion_al_round_cutoff, no_suggestion_al_round_cutoff = determine_al_round_cutoff(experiment_results, full_train_dataset, c_v, c_a)\n",
        "\n",
        "# Get the average labeling costs across experiments along with the test accuracies. Plot them.\n",
        "for (human_correct_strategy, auto_assign_strategy, b1, b2, b3, rounds, is_adaptive) in experiment_results:\n",
        "\n",
        "    # Attempt to generate a color if there is not already one.\n",
        "    if b2 == 0 and b3 == 0:\n",
        "        if (al_strategy,False) not in color_dictionary or (al_strategy,True) not in color_dictionary:\n",
        "            raise ValueError(F\"Add color for {al_strategy}\")\n",
        "        else:\n",
        "            no_suggestion_draw_color = color_dictionary[(al_strategy,False)]\n",
        "            suggestion_draw_color = color_dictionary[(al_strategy,True)]\n",
        "\n",
        "            # Draw the suggest AL baseline\n",
        "            exp_results_list = experiment_results[(human_correct_strategy, auto_assign_strategy, b1, b2, b3, rounds, is_adaptive)]\n",
        "            print(F\"RUN COUNT {dataset} {al_strategy} {human_correct_strategy} {auto_assign_strategy} {b1} {b2} {b3} {seed_size} {rounds} {is_adaptive}: {len(exp_results_list)}\")\n",
        "            labeling_costs = get_avg_labeling_costs(exp_results_list, c_v, c_a, True, full_train_dataset)\n",
        "            average_acc, std = get_avg_std_test_acc(exp_results_list)\n",
        "            lower_list = [(x-y) for (x,y) in zip(average_acc,std)]\n",
        "            upper_list = [(x+y) for (x,y) in zip(average_acc,std)]\n",
        "            \n",
        "            line = axes[3][0].plot(labeling_costs[:suggestion_al_round_cutoff], average_acc[:suggestion_al_round_cutoff], color=suggestion_draw_color, marker='o')[0]\n",
        "            axes[3][0].fill_between(labeling_costs[:suggestion_al_round_cutoff], lower_list[:suggestion_al_round_cutoff], upper_list[:suggestion_al_round_cutoff], alpha=shade_alpha, color=suggestion_draw_color)\n",
        "\n",
        "            if (al_strategy, True) not in legend_line_dictionary:\n",
        "                legend_line_dictionary[(al_strategy, True)] = line\n",
        "\n",
        "            # Draw the no-suggest AL baseline\n",
        "            labeling_costs = get_avg_labeling_costs(exp_results_list, c_v, c_a, False, full_train_dataset)\n",
        "            average_acc, std = get_avg_std_test_acc(exp_results_list)\n",
        "            lower_list = [(x-y) for (x,y) in zip(average_acc,std)]\n",
        "            upper_list = [(x+y) for (x,y) in zip(average_acc,std)]\n",
        "            \n",
        "            line = axes[3][0].plot(labeling_costs[:no_suggestion_al_round_cutoff], average_acc[:no_suggestion_al_round_cutoff], color=no_suggestion_draw_color, marker='o')[0]\n",
        "            axes[3][0].fill_between(labeling_costs[:no_suggestion_al_round_cutoff], lower_list[:no_suggestion_al_round_cutoff], upper_list[:no_suggestion_al_round_cutoff], alpha=shade_alpha, color=no_suggestion_draw_color)\n",
        "\n",
        "            if (al_strategy, False) not in legend_line_dictionary:\n",
        "                legend_line_dictionary[(al_strategy, False)] = line\n",
        "\n",
        "            al_key = (human_correct_strategy, auto_assign_strategy, b1, b2, b3, rounds, is_adaptive)\n",
        "    else:\n",
        "        if (al_strategy, human_correct_strategy, auto_assign_strategy) not in color_dictionary:\n",
        "            raise ValueError(F\"Add color for {al_strategy}+{human_correct_strategy}+{auto_assign_strategy}\")\n",
        "        else:\n",
        "            draw_color = color_dictionary[(al_strategy, human_correct_strategy, auto_assign_strategy)]\n",
        "\n",
        "        exp_results_list = experiment_results[(human_correct_strategy, auto_assign_strategy, b1, b2, b3, rounds, is_adaptive)]\n",
        "        print(F\"RUN COUNT {dataset} {al_strategy} {human_correct_strategy} {auto_assign_strategy} {b1} {b2} {b3} {seed_size} {rounds} {is_adaptive}: {len(exp_results_list)}\")\n",
        "        labeling_costs = get_avg_labeling_costs(exp_results_list, c_v, c_a, True, full_train_dataset)\n",
        "        average_acc, std = get_avg_std_test_acc(exp_results_list)\n",
        "        lower_list = [(x-y) for (x,y) in zip(average_acc,std)]\n",
        "        upper_list = [(x+y) for (x,y) in zip(average_acc,std)]\n",
        "\n",
        "        line = axes[3][0].plot(labeling_costs, average_acc, color=draw_color, marker='o')[0]\n",
        "        axes[3][0].fill_between(labeling_costs, lower_list, upper_list, alpha=shade_alpha, color=draw_color)\n",
        "\n",
        "        if (al_strategy, human_correct_strategy, auto_assign_strategy) not in legend_line_dictionary:\n",
        "            legend_line_dictionary[(al_strategy, human_correct_strategy, auto_assign_strategy)] = line\n",
        "\n",
        "\n",
        "# Do the same for the labeling efficiency.\n",
        "al_exp_results_list = experiment_results[al_key]\n",
        "al_labeling_costs = get_avg_labeling_costs(al_exp_results_list, c_v, c_a, False, full_train_dataset)\n",
        "al_average_acc, _ = get_avg_std_test_acc(al_exp_results_list)\n",
        "\n",
        "for (human_correct_strategy, auto_assign_strategy, b1, b2, b3, rounds, is_adaptive) in experiment_results:\n",
        "\n",
        "    # If this is the AL key, plot the gain with suggestion over no suggestion.\n",
        "    if b2 == 0 and b3 == 0:\n",
        "        \n",
        "        suggestion_draw_color = color_dictionary[(al_strategy, True)]\n",
        "        exp_results_list = experiment_results[(human_correct_strategy, auto_assign_strategy, b1, b2, b3, rounds, is_adaptive)]\n",
        "        labeling_costs = get_avg_labeling_costs(exp_results_list, c_v, c_a, True, full_train_dataset)\n",
        "        average_acc, _ = get_avg_std_test_acc(exp_results_list)\n",
        "\n",
        "        plot_test_accuracies, labeling_efficiencies = produce_label_efficiency_data(al_labeling_costs[:suggestion_al_round_cutoff], al_average_acc[:suggestion_al_round_cutoff], labeling_costs[:suggestion_al_round_cutoff], average_acc[:suggestion_al_round_cutoff], label_eff_acc_granularity)\n",
        "\n",
        "        axes[3][2].plot(plot_test_accuracies, labeling_efficiencies, color=suggestion_draw_color, marker='o')[0]\n",
        "    else:\n",
        "\n",
        "        draw_color = color_dictionary[(al_strategy, human_correct_strategy, auto_assign_strategy)]\n",
        "        exp_results_list = experiment_results[(human_correct_strategy, auto_assign_strategy, b1, b2, b3, rounds, is_adaptive)]\n",
        "        labeling_costs = get_avg_labeling_costs(exp_results_list, c_v, c_a, True, full_train_dataset)\n",
        "        average_acc, _ = get_avg_std_test_acc(exp_results_list)\n",
        "\n",
        "        plot_test_accuracies, labeling_efficiencies = produce_label_efficiency_data(al_labeling_costs, al_average_acc, labeling_costs, average_acc, label_eff_acc_granularity)\n",
        "\n",
        "        axes[3][2].plot(plot_test_accuracies, labeling_efficiencies, color=draw_color, marker='o')[0]\n",
        "\n",
        "# ===== ENTROPY =====\n",
        "al_strategy = \"entropy\"\n",
        "del al_key\n",
        "\n",
        "# Get CIFAR-10 results.\n",
        "results_directory = os.path.join(base_save_directory, dataset, al_strategy, str(seed_size))\n",
        "experiment_results = get_experiment_results(results_directory, dataset, al_strategy, seed_size, b_configs)\n",
        "suggestion_al_round_cutoff, no_suggestion_al_round_cutoff = determine_al_round_cutoff(experiment_results, full_train_dataset, c_v, c_a)\n",
        "\n",
        "# Get the average labeling costs across experiments along with the test accuracies. Plot them.\n",
        "for (human_correct_strategy, auto_assign_strategy, b1, b2, b3, rounds, is_adaptive) in experiment_results:\n",
        "\n",
        "    # Attempt to generate a color if there is not already one.\n",
        "    if b2 == 0 and b3 == 0:\n",
        "        if (al_strategy,False) not in color_dictionary or (al_strategy,True) not in color_dictionary:\n",
        "            raise ValueError(F\"Add color for {al_strategy}\")\n",
        "        else:\n",
        "            no_suggestion_draw_color = color_dictionary[(al_strategy,False)]\n",
        "            suggestion_draw_color = color_dictionary[(al_strategy,True)]\n",
        "\n",
        "            # Draw the suggest AL baseline\n",
        "            exp_results_list = experiment_results[(human_correct_strategy, auto_assign_strategy, b1, b2, b3, rounds, is_adaptive)]\n",
        "            print(F\"RUN COUNT {dataset} {al_strategy} {human_correct_strategy} {auto_assign_strategy} {b1} {b2} {b3} {seed_size} {rounds} {is_adaptive}: {len(exp_results_list)}\")\n",
        "            labeling_costs = get_avg_labeling_costs(exp_results_list, c_v, c_a, True, full_train_dataset)\n",
        "            average_acc, std = get_avg_std_test_acc(exp_results_list)\n",
        "            lower_list = [(x-y) for (x,y) in zip(average_acc,std)]\n",
        "            upper_list = [(x+y) for (x,y) in zip(average_acc,std)]\n",
        "            \n",
        "            line = axes[3][1].plot(labeling_costs[:suggestion_al_round_cutoff], average_acc[:suggestion_al_round_cutoff], color=suggestion_draw_color, marker='o')[0]\n",
        "            axes[3][1].fill_between(labeling_costs[:suggestion_al_round_cutoff], lower_list[:suggestion_al_round_cutoff], upper_list[:suggestion_al_round_cutoff], alpha=shade_alpha, color=suggestion_draw_color)\n",
        "\n",
        "            if (al_strategy, True) not in legend_line_dictionary:\n",
        "                legend_line_dictionary[(al_strategy, True)] = line\n",
        "\n",
        "            # Draw the no-suggest AL baseline\n",
        "            labeling_costs = get_avg_labeling_costs(exp_results_list, c_v, c_a, False, full_train_dataset)\n",
        "            average_acc, std = get_avg_std_test_acc(exp_results_list)\n",
        "            lower_list = [(x-y) for (x,y) in zip(average_acc,std)]\n",
        "            upper_list = [(x+y) for (x,y) in zip(average_acc,std)]\n",
        "            \n",
        "            line = axes[3][1].plot(labeling_costs[:no_suggestion_al_round_cutoff], average_acc[:no_suggestion_al_round_cutoff], color=no_suggestion_draw_color, marker='o')[0]\n",
        "            axes[3][1].fill_between(labeling_costs[:no_suggestion_al_round_cutoff], lower_list[:no_suggestion_al_round_cutoff], upper_list[:no_suggestion_al_round_cutoff], alpha=shade_alpha, color=no_suggestion_draw_color)\n",
        "\n",
        "            if (al_strategy, False) not in legend_line_dictionary:\n",
        "                legend_line_dictionary[(al_strategy, False)] = line\n",
        "\n",
        "            al_key = (human_correct_strategy, auto_assign_strategy, b1, b2, b3, rounds, is_adaptive)\n",
        "    else:\n",
        "        if (al_strategy, human_correct_strategy, auto_assign_strategy) not in color_dictionary:\n",
        "            raise ValueError(F\"Add color for {al_strategy}+{human_correct_strategy}+{auto_assign_strategy}\")\n",
        "        else:\n",
        "            draw_color = color_dictionary[(al_strategy, human_correct_strategy, auto_assign_strategy)]\n",
        "\n",
        "        exp_results_list = experiment_results[(human_correct_strategy, auto_assign_strategy, b1, b2, b3, rounds, is_adaptive)]\n",
        "        print(F\"RUN COUNT {dataset} {al_strategy} {human_correct_strategy} {auto_assign_strategy} {b1} {b2} {b3} {seed_size} {rounds} {is_adaptive}: {len(exp_results_list)}\")\n",
        "        labeling_costs = get_avg_labeling_costs(exp_results_list, c_v, c_a, True, full_train_dataset)\n",
        "        average_acc, std = get_avg_std_test_acc(exp_results_list)\n",
        "        lower_list = [(x-y) for (x,y) in zip(average_acc,std)]\n",
        "        upper_list = [(x+y) for (x,y) in zip(average_acc,std)]\n",
        "\n",
        "        line = axes[3][1].plot(labeling_costs, average_acc, color=draw_color, marker='o')[0]\n",
        "        axes[3][1].fill_between(labeling_costs, lower_list, upper_list, alpha=shade_alpha, color=draw_color)\n",
        "\n",
        "        if (al_strategy, human_correct_strategy, auto_assign_strategy) not in legend_line_dictionary:\n",
        "            legend_line_dictionary[(al_strategy, human_correct_strategy, auto_assign_strategy)] = line\n",
        "\n",
        "\n",
        "# Do the same for the labeling efficiency.\n",
        "al_exp_results_list = experiment_results[al_key]\n",
        "al_labeling_costs = get_avg_labeling_costs(al_exp_results_list, c_v, c_a, False, full_train_dataset)\n",
        "al_average_acc, _ = get_avg_std_test_acc(al_exp_results_list)\n",
        "\n",
        "for (human_correct_strategy, auto_assign_strategy, b1, b2, b3, rounds, is_adaptive) in experiment_results:\n",
        "\n",
        "    # If this is the AL key, plot the gain with suggestion over no suggestion.\n",
        "    if b2 == 0 and b3 == 0:\n",
        "        \n",
        "        suggestion_draw_color = color_dictionary[(al_strategy, True)]\n",
        "        exp_results_list = experiment_results[(human_correct_strategy, auto_assign_strategy, b1, b2, b3, rounds, is_adaptive)]\n",
        "        labeling_costs = get_avg_labeling_costs(exp_results_list, c_v, c_a, True, full_train_dataset)\n",
        "        average_acc, _ = get_avg_std_test_acc(exp_results_list)\n",
        "\n",
        "        plot_test_accuracies, labeling_efficiencies = produce_label_efficiency_data(al_labeling_costs[:suggestion_al_round_cutoff], al_average_acc[:suggestion_al_round_cutoff], labeling_costs[:suggestion_al_round_cutoff], average_acc[:suggestion_al_round_cutoff], label_eff_acc_granularity)\n",
        "\n",
        "        axes[3][3].plot(plot_test_accuracies, labeling_efficiencies, color=suggestion_draw_color, marker='o')[0]\n",
        "    else:\n",
        "\n",
        "        draw_color = color_dictionary[(al_strategy, human_correct_strategy, auto_assign_strategy)]\n",
        "        exp_results_list = experiment_results[(human_correct_strategy, auto_assign_strategy, b1, b2, b3, rounds, is_adaptive)]\n",
        "        labeling_costs = get_avg_labeling_costs(exp_results_list, c_v, c_a, True, full_train_dataset)\n",
        "        average_acc, _ = get_avg_std_test_acc(exp_results_list)\n",
        "\n",
        "        plot_test_accuracies, labeling_efficiencies = produce_label_efficiency_data(al_labeling_costs, al_average_acc, labeling_costs, average_acc, label_eff_acc_granularity)\n",
        "\n",
        "        axes[3][3].plot(plot_test_accuracies, labeling_efficiencies, color=draw_color, marker='o')[0]\n",
        "\n",
        "# =========================================================================================================================================================\n",
        "#\n",
        "# =================================================================== FINISH PLOT =========================================================================\n",
        "#\n",
        "# =========================================================================================================================================================\n",
        "\n",
        "# Create the legend by obtaining the list of labels\n",
        "label_list = []\n",
        "line_list = []\n",
        "for key in legend_line_dictionary:\n",
        "    line_list.append(legend_line_dictionary[key])\n",
        "    if len(key) == 2:\n",
        "        if key[1]:\n",
        "            label_list.append(r\"\\textsc{\" + F\"{key[0].capitalize()}\" + r\" w/ Suggest}\")\n",
        "        else:\n",
        "            label_list.append(r\"\\textsc{\" + F\"{key[0].capitalize()}\" + r\" w/o Suggest}\")\n",
        "    else:\n",
        "        al_strategy = key[0].replace(\"_\", \"-\").capitalize()\n",
        "        human_correct_strategy = key[1].replace(\"_\", \"-\").capitalize()\n",
        "        auto_assign_strategy = key[2].replace(\"_\", \"-\").capitalize()\n",
        "        label_list.append(r\"\\textsc{\" + F\"{al_strategy} w/ {acronym}\" + r\"}\")\n",
        "\n",
        "permutation = [1,5,0,4,2,3]\n",
        "label_list = [label_list[i] for i in permutation]\n",
        "line_list = [line_list[i] for i in permutation]\n",
        "comparison_fig.legend(line_list, label_list, loc=\"upper center\", ncol=3, borderaxespad=0, fontsize=legend_font_size)"
      ],
      "metadata": {
        "id": "6l7t2sOCCgF3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Suggested Label Accuracy\n",
        "\n",
        "Currently, a 3 row, 4 column figure showcasing the suggestion accuracy of each component."
      ],
      "metadata": {
        "id": "MMDVnOnY6iSZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "nrows = 2\n",
        "ncols = 3\n",
        "\n",
        "figsize = (5.5 * ncols, 4.5 * nrows)\n",
        "comparison_fig, axes = plt.subplots(nrows=nrows, ncols=ncols, figsize=figsize, dpi=120, gridspec_kw = {'top':0.90,'wspace':0.35,'hspace':0.35})\n",
        "\n",
        "# Note: Figures are not to have titles. The caption serves this purpose.\n",
        "\n",
        "# Set error bar alpha\n",
        "shade_alpha = 0.25\n",
        "\n",
        "legend_line_dictionary = {}\n",
        "\n",
        "# =========================================================================================================================================================\n",
        "#\n",
        "# ====================================================================== DOGS ==========================================================================\n",
        "#\n",
        "# =========================================================================================================================================================\n",
        "\n",
        "dataset = \"dogs\"\n",
        "seed_size = 500\n",
        "b_configs = [(100,600,300,False)]\n",
        "\n",
        "c_a = 4\n",
        "c_v = 1\n",
        "\n",
        "# Get Dogs dataset.\n",
        "full_train_dataset = DogsDataset(dataset_root_path, download=True, train=True)\n",
        "test_dataset = DogsDataset(dataset_root_path, download=True, train=False)\n",
        "nclasses = 120\n",
        "\n",
        "# Set the subplot title, y-axis, x-axis\n",
        "axes[0][0].set_title(r\"\\textbf{Stanford Dogs}\", fontsize=plot_title_font_size)\n",
        "axes[0][0].set_xlabel(r\"\\textbf{Round}\", fontsize=axis_label_font_size)\n",
        "axes[0][0].set_ylabel(r\"\\textbf{AL Acc. (\" + F\"{100},\" + F\"{600},\" + F\"{300})\" + r\"}\", fontsize=axis_label_font_size)\n",
        "\n",
        "axes[0][1].set_title(r\"\\textbf{Stanford Dogs}\", fontsize=plot_title_font_size)\n",
        "axes[0][1].set_xlabel(r\"\\textbf{Round}\", fontsize=axis_label_font_size)\n",
        "axes[0][1].set_ylabel(r\"\\textbf{SMI Acc. (\" + F\"{100},\" + F\"{600},\" + F\"{300})\" + r\"}\", fontsize=axis_label_font_size)\n",
        "\n",
        "axes[0][2].set_title(r\"\\textbf{Stanford Dogs}\", fontsize=plot_title_font_size)\n",
        "axes[0][2].set_xlabel(r\"\\textbf{Round}\", fontsize=axis_label_font_size)\n",
        "axes[0][2].set_ylabel(r\"\\textbf{Auto Acc. (\" + F\"{100},\" + F\"{600},\" + F\"{300})\" + r\"}\", fontsize=axis_label_font_size)\n",
        "\n",
        "# ===== BADGE =====\n",
        "al_strategy = \"badge\"\n",
        "\n",
        "# Get CIFAR-10 results.\n",
        "results_directory = os.path.join(base_save_directory, dataset, al_strategy, str(seed_size))\n",
        "experiment_results = get_experiment_results(results_directory, dataset, al_strategy, seed_size, b_configs)\n",
        "\n",
        "# Get the average suggestion accuracies across experiments along with the test accuracies. Plot them.\n",
        "for (human_correct_strategy, auto_assign_strategy, b1, b2, b3, rounds, is_adaptive) in experiment_results:\n",
        "   \n",
        "    exp_results_list = experiment_results[(human_correct_strategy, auto_assign_strategy, b1, b2, b3, rounds, is_adaptive)]\n",
        "\n",
        "    # Get row number\n",
        "    draw_color = color_dictionary[(al_strategy, human_correct_strategy, auto_assign_strategy)]\n",
        "    round_count = len(exp_results_list[0]['set_sizes']) - 1\n",
        "\n",
        "    round_numbers = [i+1 for i in range(round_count)]\n",
        "\n",
        "    al_labeled_acc, al_labeled_std = get_avg_std_correctly_suggested_al_points(exp_results_list, full_train_dataset)\n",
        "    lower_list = [(x-y) for (x,y) in zip(al_labeled_acc,al_labeled_std)]\n",
        "    upper_list = [(x+y) for (x,y) in zip(al_labeled_acc,al_labeled_std)]\n",
        "    axes[0][0].plot(round_numbers, al_labeled_acc, color=draw_color, marker='o')\n",
        "    axes[0][0].fill_between(round_numbers, lower_list, upper_list, alpha=shade_alpha, color=draw_color)\n",
        "\n",
        "    human_correct_acc, human_correct_std = get_avg_std_correctly_suggested_human_corrected_points(exp_results_list)\n",
        "    lower_list = [(x-y) for (x,y) in zip(human_correct_acc,human_correct_std)]\n",
        "    upper_list = [(x+y) for (x,y) in zip(human_correct_acc,human_correct_std)]\n",
        "    axes[0][1].plot(round_numbers, human_correct_acc, color=draw_color, marker='o')\n",
        "    axes[0][1].fill_between(round_numbers, lower_list, upper_list, alpha=shade_alpha, color=draw_color)\n",
        "\n",
        "    auto_acc, auto_std = get_avg_std_correctly_auto_labeled_points(exp_results_list)\n",
        "    lower_list = [(x-y) for (x,y) in zip(auto_acc, auto_std)]\n",
        "    upper_list = [(x+y) for (x,y) in zip(auto_acc, auto_std)]\n",
        "    line = axes[0][2].plot(round_numbers, auto_acc, color=draw_color, marker='o')[0]\n",
        "    axes[0][2].fill_between(round_numbers, lower_list, upper_list, alpha=shade_alpha, color=draw_color)\n",
        "\n",
        "    if (al_strategy, human_correct_strategy, auto_assign_strategy) not in legend_line_dictionary:\n",
        "        legend_line_dictionary[(al_strategy, human_correct_strategy, auto_assign_strategy)] = line\n",
        "\n",
        "# ===== ENTROPY =====\n",
        "al_strategy = \"entropy\"\n",
        "\n",
        "# Get CIFAR-10 results.\n",
        "results_directory = os.path.join(base_save_directory, dataset, al_strategy, str(seed_size))\n",
        "experiment_results = get_experiment_results(results_directory, dataset, al_strategy, seed_size, b_configs)\n",
        "\n",
        "# Get the average suggestion accuracies across experiments along with the test accuracies. Plot them.\n",
        "for (human_correct_strategy, auto_assign_strategy, b1, b2, b3, rounds, is_adaptive) in experiment_results:\n",
        "   \n",
        "    exp_results_list = experiment_results[(human_correct_strategy, auto_assign_strategy, b1, b2, b3, rounds, is_adaptive)]\n",
        "\n",
        "    # Get row number\n",
        "    draw_color = color_dictionary[(al_strategy, human_correct_strategy, auto_assign_strategy)]\n",
        "    round_count = len(exp_results_list[0]['set_sizes']) - 1\n",
        "\n",
        "    round_numbers = [i+1 for i in range(round_count)]\n",
        "\n",
        "    al_labeled_acc, al_labeled_std = get_avg_std_correctly_suggested_al_points(exp_results_list, full_train_dataset)\n",
        "    lower_list = [(x-y) for (x,y) in zip(al_labeled_acc,al_labeled_std)]\n",
        "    upper_list = [(x+y) for (x,y) in zip(al_labeled_acc,al_labeled_std)]\n",
        "    axes[0][0].plot(round_numbers, al_labeled_acc, color=draw_color, marker='o')\n",
        "    axes[0][0].fill_between(round_numbers, lower_list, upper_list, alpha=shade_alpha, color=draw_color)\n",
        "\n",
        "    human_correct_acc, human_correct_std = get_avg_std_correctly_suggested_human_corrected_points(exp_results_list)\n",
        "    lower_list = [(x-y) for (x,y) in zip(human_correct_acc,human_correct_std)]\n",
        "    upper_list = [(x+y) for (x,y) in zip(human_correct_acc,human_correct_std)]\n",
        "    axes[0][1].plot(round_numbers, human_correct_acc, color=draw_color, marker='o')\n",
        "    axes[0][1].fill_between(round_numbers, lower_list, upper_list, alpha=shade_alpha, color=draw_color)\n",
        "\n",
        "    auto_acc, auto_std = get_avg_std_correctly_auto_labeled_points(exp_results_list)\n",
        "    lower_list = [(x-y) for (x,y) in zip(auto_acc, auto_std)]\n",
        "    upper_list = [(x+y) for (x,y) in zip(auto_acc, auto_std)]\n",
        "    line = axes[0][2].plot(round_numbers, auto_acc, color=draw_color, marker='o')[0]\n",
        "    axes[0][2].fill_between(round_numbers, lower_list, upper_list, alpha=shade_alpha, color=draw_color)\n",
        "\n",
        "    if (al_strategy, human_correct_strategy, auto_assign_strategy) not in legend_line_dictionary:\n",
        "        legend_line_dictionary[(al_strategy, human_correct_strategy, auto_assign_strategy)] = line\n",
        "\n",
        "# =========================================================================================================================================================\n",
        "#\n",
        "# ======================================================================= BIRDS ===========================================================================\n",
        "#\n",
        "# =========================================================================================================================================================\n",
        "\n",
        "# Note: b_configs is a list of tuples of the form (b_1, b_2, b_3, is_adaptive).\n",
        "dataset = \"birds\"\n",
        "seed_size = 600\n",
        "b_configs = [(60,400,140,False)]\n",
        "\n",
        "# Get CUB dataset.\n",
        "full_train_dataset = BirdsDataset(dataset_root_path, download=True, train=True)\n",
        "test_dataset = BirdsDataset(dataset_root_path, download=True, train=False)\n",
        "nclasses = 200\n",
        "    \n",
        "# Set the subplot title, y-axis, x-axis\n",
        "axes[1][0].set_title(r\"\\textbf{CUB-200-2011}\", fontsize=plot_title_font_size)\n",
        "axes[1][0].set_xlabel(r\"\\textbf{Round}\", fontsize=axis_label_font_size)\n",
        "axes[1][0].set_ylabel(r\"\\textbf{AL Acc. (\" + F\"{60},\" + F\"{400},\" + F\"{140})\" + r\"}\", fontsize=axis_label_font_size)\n",
        "\n",
        "axes[1][1].set_title(r\"\\textbf{CUB-200-2011}\", fontsize=plot_title_font_size)\n",
        "axes[1][1].set_xlabel(r\"\\textbf{Round}\", fontsize=axis_label_font_size)\n",
        "axes[1][1].set_ylabel(r\"\\textbf{SMI Acc. (\" + F\"{60},\" + F\"{400},\" + F\"{140})\" + r\"}\", fontsize=axis_label_font_size)\n",
        "\n",
        "axes[1][2].set_title(r\"\\textbf{CUB-200-2011}\", fontsize=plot_title_font_size)\n",
        "axes[1][2].set_xlabel(r\"\\textbf{Round}\", fontsize=axis_label_font_size)\n",
        "axes[1][2].set_ylabel(r\"\\textbf{Auto Acc. (\" + F\"{60},\" + F\"{400},\" + F\"{140})\" + r\"}\", fontsize=axis_label_font_size)\n",
        "\n",
        "# ===== BADGE =====\n",
        "al_strategy = \"badge\"\n",
        "\n",
        "# Get CIFAR-10 results.\n",
        "results_directory = os.path.join(base_save_directory, dataset, al_strategy, str(seed_size))\n",
        "experiment_results = get_experiment_results(results_directory, dataset, al_strategy, seed_size, b_configs)\n",
        "\n",
        "# Get the average suggestion accuracies across experiments along with the test accuracies. Plot them.\n",
        "for (human_correct_strategy, auto_assign_strategy, b1, b2, b3, rounds, is_adaptive) in experiment_results:\n",
        "   \n",
        "    exp_results_list = experiment_results[(human_correct_strategy, auto_assign_strategy, b1, b2, b3, rounds, is_adaptive)]\n",
        "\n",
        "    # Get row number\n",
        "    draw_color = color_dictionary[(al_strategy, human_correct_strategy, auto_assign_strategy)]\n",
        "    round_count = len(exp_results_list[0]['set_sizes']) - 1\n",
        "\n",
        "    round_numbers = [i+1 for i in range(round_count)]\n",
        "\n",
        "    al_labeled_acc, al_labeled_std = get_avg_std_correctly_suggested_al_points(exp_results_list, full_train_dataset)\n",
        "    lower_list = [(x-y) for (x,y) in zip(al_labeled_acc,al_labeled_std)]\n",
        "    upper_list = [(x+y) for (x,y) in zip(al_labeled_acc,al_labeled_std)]\n",
        "    axes[1][0].plot(round_numbers, al_labeled_acc, color=draw_color, marker='o')\n",
        "    axes[1][0].fill_between(round_numbers, lower_list, upper_list, alpha=shade_alpha, color=draw_color)\n",
        "\n",
        "    human_correct_acc, human_correct_std = get_avg_std_correctly_suggested_human_corrected_points(exp_results_list)\n",
        "    lower_list = [(x-y) for (x,y) in zip(human_correct_acc,human_correct_std)]\n",
        "    upper_list = [(x+y) for (x,y) in zip(human_correct_acc,human_correct_std)]\n",
        "    axes[1][1].plot(round_numbers, human_correct_acc, color=draw_color, marker='o')\n",
        "    axes[1][1].fill_between(round_numbers, lower_list, upper_list, alpha=shade_alpha, color=draw_color)\n",
        "\n",
        "    auto_acc, auto_std = get_avg_std_correctly_auto_labeled_points(exp_results_list)\n",
        "    lower_list = [(x-y) for (x,y) in zip(auto_acc, auto_std)]\n",
        "    upper_list = [(x+y) for (x,y) in zip(auto_acc, auto_std)]\n",
        "    line = axes[1][2].plot(round_numbers, auto_acc, color=draw_color, marker='o')[0]\n",
        "    axes[1][2].fill_between(round_numbers, lower_list, upper_list, alpha=shade_alpha, color=draw_color)\n",
        "\n",
        "    if (al_strategy, human_correct_strategy, auto_assign_strategy) not in legend_line_dictionary:\n",
        "        legend_line_dictionary[(al_strategy, human_correct_strategy, auto_assign_strategy)] = line\n",
        "\n",
        "# ===== ENTROPY =====\n",
        "al_strategy = \"entropy\"\n",
        "\n",
        "# Get CIFAR-10 results.\n",
        "results_directory = os.path.join(base_save_directory, dataset, al_strategy, str(seed_size))\n",
        "experiment_results = get_experiment_results(results_directory, dataset, al_strategy, seed_size, b_configs)\n",
        "\n",
        "# Get the average suggestion accuracies across experiments along with the test accuracies. Plot them.\n",
        "for (human_correct_strategy, auto_assign_strategy, b1, b2, b3, rounds, is_adaptive) in experiment_results:\n",
        "   \n",
        "    exp_results_list = experiment_results[(human_correct_strategy, auto_assign_strategy, b1, b2, b3, rounds, is_adaptive)]\n",
        "\n",
        "    # Get row number\n",
        "    draw_color = color_dictionary[(al_strategy, human_correct_strategy, auto_assign_strategy)]\n",
        "    round_count = len(exp_results_list[0]['set_sizes']) - 1\n",
        "\n",
        "    round_numbers = [i+1 for i in range(round_count)]\n",
        "\n",
        "    al_labeled_acc, al_labeled_std = get_avg_std_correctly_suggested_al_points(exp_results_list, full_train_dataset)\n",
        "    lower_list = [(x-y) for (x,y) in zip(al_labeled_acc,al_labeled_std)]\n",
        "    upper_list = [(x+y) for (x,y) in zip(al_labeled_acc,al_labeled_std)]\n",
        "    axes[1][0].plot(round_numbers, al_labeled_acc, color=draw_color, marker='o')\n",
        "    axes[1][0].fill_between(round_numbers, lower_list, upper_list, alpha=shade_alpha, color=draw_color)\n",
        "\n",
        "    human_correct_acc, human_correct_std = get_avg_std_correctly_suggested_human_corrected_points(exp_results_list)\n",
        "    lower_list = [(x-y) for (x,y) in zip(human_correct_acc,human_correct_std)]\n",
        "    upper_list = [(x+y) for (x,y) in zip(human_correct_acc,human_correct_std)]\n",
        "    axes[1][1].plot(round_numbers, human_correct_acc, color=draw_color, marker='o')\n",
        "    axes[1][1].fill_between(round_numbers, lower_list, upper_list, alpha=shade_alpha, color=draw_color)\n",
        "\n",
        "    auto_acc, auto_std = get_avg_std_correctly_auto_labeled_points(exp_results_list)\n",
        "    lower_list = [(x-y) for (x,y) in zip(auto_acc, auto_std)]\n",
        "    upper_list = [(x+y) for (x,y) in zip(auto_acc, auto_std)]\n",
        "    line = axes[1][2].plot(round_numbers, auto_acc, color=draw_color, marker='o')[0]\n",
        "    axes[1][2].fill_between(round_numbers, lower_list, upper_list, alpha=shade_alpha, color=draw_color)\n",
        "\n",
        "    if (al_strategy, human_correct_strategy, auto_assign_strategy) not in legend_line_dictionary:\n",
        "        legend_line_dictionary[(al_strategy, human_correct_strategy, auto_assign_strategy)] = line\n",
        "\n",
        "# Create the legend by obtaining the list of labels\n",
        "label_list = []\n",
        "line_list = []\n",
        "for key in legend_line_dictionary:\n",
        "    line_list.append(legend_line_dictionary[key])\n",
        "    if type(key) == str:\n",
        "        label_list.append(r\"\\textsc{\" + F\"{key.capitalize()}\" + r\"}\")\n",
        "    else:\n",
        "        al_strategy = key[0].replace(\"_\", \"-\").capitalize()\n",
        "        human_correct_strategy = key[1].replace(\"_\", \"-\").capitalize()\n",
        "        auto_assign_strategy = key[2].replace(\"_\", \"-\").capitalize()\n",
        "        label_list.append(r\"\\textsc{\" + F\"{al_strategy} w/ {acronym}\" + r\"}\")\n",
        "comparison_fig.legend(line_list, label_list, loc=\"upper center\", ncol=2, borderaxespad=0, fontsize=legend_font_size)    "
      ],
      "metadata": {
        "id": "nzKR4JDL6mjh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Ablation\n"
      ],
      "metadata": {
        "id": "ue6pP1iG6myL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "use_al_pseudo = True\n",
        "nrows = 2\n",
        "ncols = 4\n",
        "\n",
        "figsize = (5.5 * ncols, 5.5 * nrows)\n",
        "comparison_fig, axes = plt.subplots(nrows=nrows, ncols=ncols, figsize=figsize, dpi=120, gridspec_kw = {'top':0.92,'wspace':0.3,'hspace':0.3})\n",
        "\n",
        "# Note: Figures are not to have titles. The caption serves this purpose.\n",
        "\n",
        "# Set error bar alpha\n",
        "shade_alpha = 0.25\n",
        "\n",
        "legend_line_dictionary = {}\n",
        "\n",
        "# Seed random color generator\n",
        "np.random.seed(42)\n",
        "\n",
        "# =========================================================================================================================================================\n",
        "#\n",
        "# ===================================================================== CIFAR-100 =========================================================================\n",
        "#\n",
        "# =========================================================================================================================================================\n",
        "\n",
        "# Note: b_configs is a list of tuples of the form (b_1, b_2, b_3, is_adaptive).\n",
        "dataset = \"cifar100\"\n",
        "seed_size = 5000\n",
        "b_configs = [(0,3500,500,False),\n",
        "             (1000,0,500,False),\n",
        "             (1000,3500,0,False),\n",
        "             (0,3500,0,False)]\n",
        "base_b_config = [(1000,3500,500,False)]\n",
        "c_a = 2\n",
        "c_v = 1\n",
        "\n",
        "# Get CIFAR-100 dataset.\n",
        "full_train_dataset = datasets.CIFAR100(dataset_root_path, download=True, train=True)\n",
        "test_dataset = datasets.CIFAR100(dataset_root_path, download=True, train=False)\n",
        "nclasses = 100 # NUM CLASSES HERE\n",
        "    \n",
        "column_map = {(0,3500,500): 0,\n",
        "             (1000,0,500):  1,\n",
        "             (1000,3500,0): 2,\n",
        "             (0,3500,0):    3}\n",
        "\n",
        "# Set the subplot title, y-axis, x-axis\n",
        "axes[0][0].set_title(r\"\\textbf{No AL (CIFAR-100)}\", fontsize=plot_title_font_size)\n",
        "axes[0][0].set_xlabel(r\"\\textbf{Labeling Cost (\" + F\"{c_a},\" + F\"{c_v})\" + r\"}\", fontsize=axis_label_font_size)\n",
        "axes[0][0].set_ylabel(r\"\\textbf{ACC (\" + F\"{0},\" + F\"{3500},\" + F\"{500})\" + r\"}\", fontsize=axis_label_font_size)\n",
        "\n",
        "axes[0][1].set_title(r\"\\textbf{No SMI (CIFAR-100)}\", fontsize=plot_title_font_size)\n",
        "axes[0][1].set_xlabel(r\"\\textbf{Labeling Cost (\" + F\"{c_a},\" + F\"{c_v})\" + r\"}\", fontsize=axis_label_font_size)\n",
        "axes[0][1].set_ylabel(r\"\\textbf{ACC (\" + F\"{1000},\" + F\"{0},\" + F\"{500})\" + r\"}\", fontsize=axis_label_font_size)\n",
        "\n",
        "axes[0][2].set_title(r\"\\textbf{No Auto (CIFAR-100)}\", fontsize=plot_title_font_size)\n",
        "axes[0][2].set_xlabel(r\"\\textbf{Labeling Cost (\" + F\"{c_a},\" + F\"{c_v})\" + r\"}\", fontsize=axis_label_font_size)\n",
        "axes[0][2].set_ylabel(r\"\\textbf{ACC (\" + F\"{1000},\" + F\"{3500},\" + F\"{0})\" + r\"}\", fontsize=axis_label_font_size)\n",
        "\n",
        "axes[0][3].set_title(r\"\\textbf{Only SMI (CIFAR-100)}\", fontsize=plot_title_font_size)\n",
        "axes[0][3].set_xlabel(r\"\\textbf{Labeling Cost (\" + F\"{c_a},\" + F\"{c_v})\" + r\"}\", fontsize=axis_label_font_size)\n",
        "axes[0][3].set_ylabel(r\"\\textbf{ACC (\" + F\"{0},\" + F\"{3500},\" + F\"{0})\" + r\"}\", fontsize=axis_label_font_size)\n",
        "\n",
        "al_strategy = \"entropy\"\n",
        "\n",
        "# Get CIFAR-10 results.\n",
        "results_directory = os.path.join(base_save_directory, dataset, al_strategy, str(seed_size))\n",
        "experiment_results = get_experiment_results(results_directory, dataset, al_strategy, seed_size, b_configs)\n",
        "base_experiment_results = get_experiment_results(results_directory, dataset, al_strategy, seed_size, base_b_config)\n",
        "\n",
        "# Get the average labeling costs across experiments along with the test accuracies. Plot them.\n",
        "for (human_correct_strategy, auto_assign_strategy, b1, b2, b3, rounds, is_adaptive) in experiment_results:\n",
        "\n",
        "    if (al_strategy, human_correct_strategy, auto_assign_strategy) not in color_dictionary:\n",
        "        raise ValueError(F\"Add color for {al_strategy}+{human_correct_strategy}+{auto_assign_strategy}\")\n",
        "    else:\n",
        "        draw_color = color_dictionary[(al_strategy, human_correct_strategy, auto_assign_strategy)]\n",
        "\n",
        "    col_num = column_map[(b1,b2,b3)]\n",
        "\n",
        "    exp_results_list = experiment_results[(human_correct_strategy, auto_assign_strategy, b1, b2, b3, rounds, is_adaptive)]\n",
        "    labeling_costs = get_avg_labeling_costs(exp_results_list, c_v, c_a, use_al_pseudo, full_train_dataset)\n",
        "    average_acc, std = get_avg_std_test_acc(exp_results_list)\n",
        "    lower_list = [(x-y) for (x,y) in zip(average_acc,std)]\n",
        "    upper_list = [(x+y) for (x,y) in zip(average_acc,std)]\n",
        "\n",
        "    line = axes[0][col_num].plot(labeling_costs, average_acc, color=(0,0,0), marker='o')[0]\n",
        "    axes[0][col_num].fill_between(labeling_costs, lower_list, upper_list, alpha=shade_alpha, color=(0,0,0))\n",
        "\n",
        "    # Draw base method\n",
        "    max_base_cost = labeling_costs[-1]\n",
        "    exp_results_list = base_experiment_results[list(base_experiment_results.keys())[0]]\n",
        "    labeling_costs = get_avg_labeling_costs(exp_results_list, c_v, c_a, use_al_pseudo, full_train_dataset)\n",
        "    for i, labeling_cost in enumerate(labeling_costs):\n",
        "        if labeling_cost > max_base_cost:\n",
        "            max_display = i\n",
        "            if b2 == 0:\n",
        "                max_display += 1\n",
        "            break\n",
        "    average_acc, std = get_avg_std_test_acc(exp_results_list)\n",
        "    lower_list = [(x-y) for (x,y) in zip(average_acc,std)]\n",
        "    upper_list = [(x+y) for (x,y) in zip(average_acc,std)]\n",
        "\n",
        "    line = axes[0][col_num].plot(labeling_costs[:max_display], average_acc[:max_display], color=draw_color, marker='o', linestyle=\"--\")[0]\n",
        "    axes[0][col_num].fill_between(labeling_costs[:max_display], lower_list[:max_display], upper_list[:max_display], alpha=shade_alpha, color=draw_color)\n",
        "\n",
        "c_a = 6\n",
        "c_v = 1\n",
        "\n",
        "# Set the subplot title, y-axis, x-axis\n",
        "axes[1][0].set_title(r\"\\textbf{No AL (CIFAR-100)}\", fontsize=plot_title_font_size)\n",
        "axes[1][0].set_xlabel(r\"\\textbf{Labeling Cost (\" + F\"{c_a},\" + F\"{c_v})\" + r\"}\", fontsize=axis_label_font_size)\n",
        "axes[1][0].set_ylabel(r\"\\textbf{ACC (\" + F\"{0},\" + F\"{3500},\" + F\"{500})\" + r\"}\", fontsize=axis_label_font_size)\n",
        "\n",
        "axes[1][1].set_title(r\"\\textbf{No SMI (CIFAR-100)}\", fontsize=plot_title_font_size)\n",
        "axes[1][1].set_xlabel(r\"\\textbf{Labeling Cost (\" + F\"{c_a},\" + F\"{c_v})\" + r\"}\", fontsize=axis_label_font_size)\n",
        "axes[1][1].set_ylabel(r\"\\textbf{ACC (\" + F\"{1000},\" + F\"{0},\" + F\"{500})\" + r\"}\", fontsize=axis_label_font_size)\n",
        "\n",
        "axes[1][2].set_title(r\"\\textbf{No Auto (CIFAR-100)}\", fontsize=plot_title_font_size)\n",
        "axes[1][2].set_xlabel(r\"\\textbf{Labeling Cost (\" + F\"{c_a},\" + F\"{c_v})\" + r\"}\", fontsize=axis_label_font_size)\n",
        "axes[1][2].set_ylabel(r\"\\textbf{ACC (\" + F\"{1000},\" + F\"{3500},\" + F\"{0})\" + r\"}\", fontsize=axis_label_font_size)\n",
        "\n",
        "axes[1][3].set_title(r\"\\textbf{Only SMI (CIFAR-100)}\", fontsize=plot_title_font_size)\n",
        "axes[1][3].set_xlabel(r\"\\textbf{Labeling Cost (\" + F\"{c_a},\" + F\"{c_v})\" + r\"}\", fontsize=axis_label_font_size)\n",
        "axes[1][3].set_ylabel(r\"\\textbf{ACC (\" + F\"{0},\" + F\"{3500},\" + F\"{0})\" + r\"}\", fontsize=axis_label_font_size)\n",
        "\n",
        "al_strategy = \"entropy\"\n",
        "\n",
        "# Get CIFAR-10 results.\n",
        "results_directory = os.path.join(base_save_directory, dataset, al_strategy, str(seed_size))\n",
        "experiment_results = get_experiment_results(results_directory, dataset, al_strategy, seed_size, b_configs)\n",
        "base_experiment_results = get_experiment_results(results_directory, dataset, al_strategy, seed_size, base_b_config)\n",
        "\n",
        "# Get the average labeling costs across experiments along with the test accuracies. Plot them.\n",
        "for (human_correct_strategy, auto_assign_strategy, b1, b2, b3, rounds, is_adaptive) in experiment_results:\n",
        "\n",
        "    if (al_strategy, human_correct_strategy, auto_assign_strategy) not in color_dictionary:\n",
        "        raise ValueError(F\"Add color for {al_strategy}+{human_correct_strategy}+{auto_assign_strategy}\")\n",
        "    else:\n",
        "        draw_color = color_dictionary[(al_strategy, human_correct_strategy, auto_assign_strategy)]\n",
        "\n",
        "    col_num = column_map[(b1,b2,b3)]\n",
        "\n",
        "    exp_results_list = experiment_results[(human_correct_strategy, auto_assign_strategy, b1, b2, b3, rounds, is_adaptive)]\n",
        "    labeling_costs = get_avg_labeling_costs(exp_results_list, c_v, c_a, use_al_pseudo, full_train_dataset)\n",
        "    average_acc, std = get_avg_std_test_acc(exp_results_list)\n",
        "    lower_list = [(x-y) for (x,y) in zip(average_acc,std)]\n",
        "    upper_list = [(x+y) for (x,y) in zip(average_acc,std)]\n",
        "\n",
        "    line = axes[1][col_num].plot(labeling_costs, average_acc, color=(0,0,0), marker='o')[0]\n",
        "    axes[1][col_num].fill_between(labeling_costs, lower_list, upper_list, alpha=shade_alpha, color=(0,0,0))\n",
        "    \n",
        "    if (al_strategy, human_correct_strategy, auto_assign_strategy) not in legend_line_dictionary:\n",
        "        legend_line_dictionary[(al_strategy, human_correct_strategy, auto_assign_strategy)] = line\n",
        "    \n",
        "    # Draw base method\n",
        "    max_base_cost = labeling_costs[-1]\n",
        "    exp_results_list = base_experiment_results[list(base_experiment_results.keys())[0]]\n",
        "    labeling_costs = get_avg_labeling_costs(exp_results_list, c_v, c_a, use_al_pseudo, full_train_dataset)\n",
        "    for i, labeling_cost in enumerate(labeling_costs):\n",
        "        if labeling_cost > max_base_cost:\n",
        "            max_display = i\n",
        "            break\n",
        "    average_acc, std = get_avg_std_test_acc(exp_results_list)\n",
        "    lower_list = [(x-y) for (x,y) in zip(average_acc,std)]\n",
        "    upper_list = [(x+y) for (x,y) in zip(average_acc,std)]\n",
        "\n",
        "    line = axes[1][col_num].plot(labeling_costs[:max_display], average_acc[:max_display], color=draw_color, marker='o', linestyle=\"--\")[0]\n",
        "    axes[1][col_num].fill_between(labeling_costs[:max_display], lower_list[:max_display], upper_list[:max_display], alpha=shade_alpha, color=draw_color)\n",
        "\n",
        "    if \"base\" not in legend_line_dictionary:\n",
        "        legend_line_dictionary[\"base\"] = line\n",
        "\n",
        "# Create the legend by obtaining the list of labels\n",
        "label_list = []\n",
        "line_list = []\n",
        "for key in legend_line_dictionary:\n",
        "    line_list.append(legend_line_dictionary[key])\n",
        "    if key == \"base\":\n",
        "        label_list.append(r\"\\textsc{Entropy w/ \" + F\"{acronym}\" + r\"}\")\n",
        "    else:\n",
        "        label_list.append(r\"\\textsc{Ablated Entropy w/ \" + F\"{acronym}\" + r\"}\")\n",
        "perm = [1,0]\n",
        "label_list = [label_list[i] for i in perm]\n",
        "line_list = [line_list[i] for i in perm]\n",
        "comparison_fig.legend(line_list, label_list, loc=\"upper center\", ncol=2, borderaxespad=0, fontsize=legend_font_size)    "
      ],
      "metadata": {
        "id": "vv42qPjC6nEr"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}